{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "QYuALZOG-AMq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# CV Course Project: Assignment - Image Recognition\n",
    "\n",
    "---\n",
    "\n",
    "**Group:**\n",
    "- Alumno 1: San Millan Rodrigues, Nadine (n.srodrigues@alumnos.upm.es)\n",
    "- Alumno 2: Sukhorukova, Anastasia (anastasia.s@alumnos.upm.es)\n",
    "- Alumno 3: Reyes Castro, Didier Yamil (didier.reyes.castro@alumnos.upm.es)\n",
    "\n",
    "**Course:** Computer Vision (CV) - 2025/26\n",
    "\n",
    "**Institution:** Polytechnic University of Madrid (UPM)\n",
    "\n",
    "**Date:** January 2026\n",
    "\n",
    "**for group members: OUR PROJECT SLIDES (editor rights link):** https://docs.google.com/presentation/d/1YELVYRoQ6xEikMO35GdRnY7-QVEacHZRYHQPR3g47eI/edit?usp=sharing\n",
    "\n",
    "---\n",
    "\n",
    "## Goals\n",
    "\n",
    "The goals of the assignment are:\n",
    "* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n",
    "* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n",
    "* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n",
    "* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n",
    "\n",
    "Follow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/2DDPE2zHw5dbM3G](https://drive.upm.es/s/2DDPE2zHw5dbM3G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Google Colab, uncomment the following line to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow numpy rasterio scikit-learn matplotlib keras\n",
    "# for MacOS:\n",
    "# pip install tensorflow-macos tensorflow-metal numpy rasterio scikit-learn matplotlib jupyterlab\n",
    "# pip install notebook ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Conda, create a new environment and install the required packages:\n",
    "```bash\n",
    "conda create -n cv_project python tensorflow numpy rasterio scikit-learn matplotlib jupyterlab\n",
    "conda activate cv_project\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import uuid\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "\n",
    "# External libraries\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU is available for training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.031186Z",
     "start_time": "2024-10-26T00:00:17.131476Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Load the Dataset & Image Loader\n",
    "\n",
    "Before loading the dataset, set the path to the folder containing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES_PATH = '/mnt/c/Users/didie/Desktop/main/MUIA/CV/Labs/Image_Recognition/xview_recognition/'\n",
    "IMAGES_PATH = '/Users/anastasia/Desktop/MUIA/CompVis/Project/Data/xview_recognition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.066937Z",
     "start_time": "2024-10-26T00:00:21.059126Z"
    },
    "editable": true,
    "id": "OYtqD3Oh-AMw",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GenericObject:\n",
    "    \"\"\"\n",
    "    Generic object data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category= -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    \"\"\"\n",
    "    Generic image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n",
    "        self.objects = list([])\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.153693Z",
     "start_time": "2024-10-26T00:00:21.149079Z"
    },
    "id": "I_GygShu-AMz"
   },
   "outputs": [],
   "source": [
    "categories = {0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck', 4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator', 8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container', 12: 'Pylon'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.292654Z",
     "start_time": "2024-10-26T00:00:21.205321Z"
    },
    "editable": true,
    "id": "fRBA7ReQ-AM0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def load_geoimage(filename):\n",
    "#    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "#    src_raster = rasterio.open(IMAGES_PATH + filename, 'r')\n",
    "#    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n",
    "#    input_type = src_raster.profile['dtype']\n",
    "#    input_channels = src_raster.count\n",
    "#    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "#    for band in range(input_channels):\n",
    "#        img[:, :, band] = src_raster.read(band+1)\n",
    "#    return img\n",
    "\n",
    "def load_geoimage(filename):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    \n",
    "    full_path = os.path.join(IMAGES_PATH, filename)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {full_path}\")\n",
    "    \n",
    "    src_raster = rasterio.open(full_path, 'r')\n",
    "    \n",
    "    input_type = src_raster.profile['dtype']\n",
    "    input_channels = src_raster.count\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "    \n",
    "    for band in range(input_channels):\n",
    "        img[:, :, band] = src_raster.read(band + 1)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.416449Z",
     "start_time": "2024-10-26T00:00:21.311510Z"
    },
    "editable": true,
    "id": "Orto292C-AM3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'images', 'annotations', 'categories'])\n"
     ]
    }
   ],
   "source": [
    "# Load database\n",
    "# json_file = IMAGES_PATH + 'xview_ann_train.json'\n",
    "json_file = IMAGES_PATH + '/xview_ann_train.json'\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "    print(json_data.keys())\n",
    "ifs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:22.874518Z",
     "start_time": "2024-10-26T00:00:22.204948Z"
    },
    "id": "4GjFLHs4-AM4",
    "outputId": "5581df22-d4e9-42ac-9f94-061fd8c7acd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cargo plane': 635, 'Small car': 3324, 'Bus': 1768, 'Truck': 2210, 'Motorboat': 1069, 'Fishing vessel': 706, 'Dump truck': 1236, 'Excavator': 789, 'Building': 3594, 'Helipad': 111, 'Storage tank': 1469, 'Shipping container': 1523, 'Pylon': 312}\n"
     ]
    }
   ],
   "source": [
    "counts = dict.fromkeys(categories.values(), 0)\n",
    "anns_dataset = []\n",
    "for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n",
    "    image = GenericImage(json_img['filename'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    obj.category = json_ann['category_id']\n",
    "    # Resampling strategy to reduce training time\n",
    "    counts[obj.category] += 1\n",
    "    image.add_object(obj)\n",
    "    anns_dataset.append(image)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_images(objs, batch_size, do_shuffle=False):\n",
    "    while True:\n",
    "        if do_shuffle:\n",
    "            np.random.shuffle(objs)\n",
    "        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n",
    "        for group in groups:\n",
    "            images, labels = [], []\n",
    "            for (filename, obj) in group:\n",
    "                # Load image\n",
    "                images.append(load_geoimage(filename))\n",
    "                probabilities = np.zeros(len(categories))\n",
    "                probabilities[list(categories.values()).index(obj.category)] = 1\n",
    "                labels.append(probabilities)\n",
    "            images = np.array(images).astype(np.float32)\n",
    "            labels = np.array(labels).astype(np.float32)\n",
    "            yield images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "def set_seed(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# Number of categories for classification\n",
    "NUM_CATEGORIES = len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Possible Architectures / Parameters to Tune\n",
    "\n",
    "- Normalisation of the input data (rescaling pixels from [0, 255] to [0, 1])\n",
    "- Adam tuning: learning rate, beta 1, beta 2, epsilon, amsgrad\n",
    "- Increase number of epochs\n",
    "- Adding more Dense layers\n",
    "- Change the activation functions of Dense layers: ReLU, leaky ReLU, ELU (which to choose for each layer?)\n",
    "- Change loss function from Crossentropy to Focal Loss\n",
    "- Weight Initialisation (HE initialisation)\n",
    "- Use BatchNorm layers (but where? in all layers?)\n",
    "- Dropout?, Early stopping?\n",
    "- Increase batch size (although it could generalise worst but be parallelised) (also batch size being related to learning rate: learning*N/batch)\n",
    "- Other advanced techniques seen in class: SGD with warm restarts, SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diNBB3qy-AM2"
   },
   "source": [
    "## 1 Model 0: Simple (and Bad) ffNN > Multinomial Logistic Regression Model\n",
    "\n",
    "The following architecture is used as a starting point:\n",
    "- Validation split: 10% (NO stratification)\n",
    "- Input layer: 150,528 neurons (224x224x3) > Activation function: ReLU\n",
    "- Output layer: 13 neurons (the categories to classify) > Activation function: Softmax\n",
    "- Optimiser: Adam with learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0\n",
    "- Loss function: Categorical Crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:23.656800Z",
     "start_time": "2024-10-26T00:00:23.123245Z"
    },
    "id": "NriAECvS-AM6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 16871\n",
      "Number of validation images: 1875\n"
     ]
    }
   ],
   "source": [
    "anns_train, anns_valid = train_test_split(anns_dataset, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)\n",
    "print('Number of training images: ' + str(len(anns_train)))\n",
    "print('Number of validation images: ' + str(len(anns_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:25.056806Z",
     "start_time": "2024-10-26T00:00:24.261581Z"
    },
    "id": "BNkjbY2e-AM7",
    "outputId": "47bde031-306f-464e-8e22-cc70a7fb7c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 18:43:57.252841: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2025-10-03 18:43:57.253095: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-10-03 18:43:57.253109: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "2025-10-03 18:43:57.253532: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-03 18:43:57.253570: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150528</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150528</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,956,877</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150528\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150528\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │     \u001b[38;5;34m1,956,877\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,956,877</span> (7.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,956,877\u001b[0m (7.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,956,877</span> (7.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,956,877\u001b[0m (7.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Compiling the model...')\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)), # 224x224 images with 3 channels (RGB)\n",
    "    Flatten(), # Flatten the 3D tensor to 1D\n",
    "    Activation('relu'),\n",
    "    Dense(NUM_CATEGORIES, activation='softmax') # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are helper functions that monitor the training process:\n",
    "- **ModelCheckpoint:** Save the best model based on validation accuracy.\n",
    "- **ReduceLROnPlateau:** Reduce learning rate when validation accuracy stops improving.\n",
    "- **EarlyStopping:** Stop training when validation accuracy stops improving.\n",
    "- **TerminateOnNaN:** Stop training if NaN loss is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:26.254555Z",
     "start_time": "2024-10-26T00:00:26.243908Z"
    },
    "id": "GGAJEfpB-AM8"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_checkpoint = ModelCheckpoint('model.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\n",
    "early_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\n",
    "terminate = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the ImageObjects to actual image generators for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:27.058834Z",
     "start_time": "2024-10-26T00:00:27.022627Z"
    },
    "id": "Yht-QqUH-AM8"
   },
   "outputs": [],
   "source": [
    "# Generate the list of objects from annotations\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n",
    "\n",
    "# Generators\n",
    "BATCH_SIZE = 16\n",
    "train_generator = generator_images(objs_train, BATCH_SIZE, do_shuffle=True)\n",
    "valid_generator = generator_images(objs_valid, BATCH_SIZE, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-26T00:00:27.913670Z"
    },
    "editable": true,
    "id": "TrfpdECs-AM9",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "21d89b78-d94c-442e-9bc2-517654c0b614",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTraining model...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m EPOCHS = \u001b[32m20\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_steps = \u001b[43mmath\u001b[49m.ceil(\u001b[38;5;28mlen\u001b[39m(objs_train)/BATCH_SIZE)\n\u001b[32m      5\u001b[39m valid_steps = math.ceil(\u001b[38;5;28mlen\u001b[39m(objs_valid)/BATCH_SIZE)\n\u001b[32m      7\u001b[39m h = model.fit(train_generator, \n\u001b[32m      8\u001b[39m               steps_per_epoch=train_steps, \n\u001b[32m      9\u001b[39m               validation_data=valid_generator, \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m               callbacks=callbacks, \n\u001b[32m     13\u001b[39m               verbose=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training model...')\n",
    "\n",
    "EPOCHS = 20\n",
    "train_steps = math.ceil(len(objs_train)/BATCH_SIZE)\n",
    "valid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n",
    "\n",
    "h = model.fit(train_generator, \n",
    "              steps_per_epoch=train_steps, \n",
    "              validation_data=valid_generator, \n",
    "              validation_steps=valid_steps, \n",
    "              epochs=EPOCHS, \n",
    "              callbacks=callbacks, \n",
    "              verbose=1)\n",
    "\n",
    "# Best validation model\n",
    "best_idx = int(np.argmax(h.history['val_accuracy']))\n",
    "best_value = np.max(h.history['val_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training time: ~ 108 minutes\n",
    "- GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
    "- Best validation model: Epoch 15 - Validation accuracy: 0.3941"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "8IMMO_mT-AM9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1.3 Validation\n",
    "Compute validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAanJ-V0-AM1"
   },
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(cm, categories):\n",
    "    # Draw confusion matrix\n",
    "    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n",
    "    ax = fig.add_subplot(111)\n",
    "    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.colormaps['Blues'])\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.keras')\n",
    "y_true, y_pred = [], []\n",
    "for ann in anns_valid:\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename)\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        y_true.append(obj_pred.category)\n",
    "        y_pred.append(pred_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n",
    "draw_confusion_matrix(cm, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy\n",
    "correct_samples_class = np.diag(cm).astype(float)\n",
    "total_samples_class = np.sum(cm, axis=1).astype(float)\n",
    "total_predicts_class = np.sum(cm, axis=0).astype(float)\n",
    "print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n",
    "acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n",
    "print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n",
    "acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n",
    "print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n",
    "for idx in range(len(categories)):\n",
    "    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n",
    "    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n",
    "    tp = cm[idx, idx]\n",
    "    fp = sum(cm[:, idx]) - tp\n",
    "    fn = sum(cm[idx, :]) - tp\n",
    "    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n",
    "    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n",
    "    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n",
    "    # Precision: proportion of predicted positive cases that were truly real positives.\n",
    "    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n",
    "    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n",
    "    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n",
    "    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n",
    "    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n",
    "    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n",
    "    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testing\n",
    "Trying to improve the results provided in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJr_-xCt-AM-"
   },
   "outputs": [],
   "source": [
    "anns = []\n",
    "for (dirpath, dirnames, filenames) in os.walk('../PROJECT/xview_recognition/xview_test'):\n",
    "    for filename in filenames:\n",
    "        image = GenericImage(dirpath[29:] + '/' + filename)\n",
    "        image.tile = np.array([0, 0, 224, 224])\n",
    "        obj = GenericObject()\n",
    "        obj.bb = (0, 0, 224, 224)\n",
    "        obj.category = dirpath[dirpath.rfind('/')+1:]\n",
    "        image.add_object(obj)\n",
    "        anns.append(image)\n",
    "print('Number of testing images: ' + str(len(anns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGs2zqfv-AM_"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.keras')\n",
    "predictions_data = {\"images\": {}, \"annotations\": {}}\n",
    "for idx, ann in enumerate(anns):\n",
    "    image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n",
    "    predictions_data[\"images\"][idx] = image_data\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename)\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n",
    "        predictions_data[\"annotations\"][idx] = annotation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prediction.json\", \"w\") as outfile:\n",
    "    json.dump(predictions_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model 1: \n",
    "### Trying to improve original model accuracy by scaling the data and tuning the ffNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 16871 Validation images: 1875\n",
      "Class counts: [ 592 2973 1557 1981  963  632 1118  726 3248  103 1320 1386  272]\n",
      "Class weights: {0: 2.1921777546777546, 1: 0.4365184092732024, 2: 0.8335062496912208, 3: 0.6551081427406515, 4: 1.3476315999680486, 5: 2.0534323271665045, 6: 1.1607953763588825, 7: 1.7875609239245602, 8: 0.399559492231906, 9: 12.599701269604182, 10: 0.9831585081585081, 11: 0.9363414363414363, 12: 4.771210407239819}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 19:44:40.854882: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2025-10-03 19:44:40.854902: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-10-03 19:44:40.854905: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "2025-10-03 19:44:40.854919: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-03 19:44:40.854927: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12288</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,145,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12288\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m3,145,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │         \u001b[38;5;34m1,677\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,182,093</span> (12.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,182,093\u001b[0m (12.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,181,325</span> (12.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,181,325\u001b[0m (12.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 19:44:41.319490: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN\n",
    "\n",
    "# -----------------------------\n",
    "# SETUP\n",
    "# -----------------------------\n",
    "IMAGES_PATH = '/Users/anastasia/Desktop/MUIA/CompVis/Project/Data/xview_recognition'\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Categories\n",
    "# -----------------------------\n",
    "categories = {\n",
    "    0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck',\n",
    "    4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator',\n",
    "    8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container',\n",
    "    12: 'Pylon'\n",
    "}\n",
    "NUM_CATEGORIES = len(categories)\n",
    "category_to_index = {v: k for k, v in categories.items()}\n",
    "index_to_category = {k: v for k, v in categories.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# Data classes\n",
    "# -----------------------------\n",
    "class GenericObject:\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category = -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])\n",
    "        self.objects = []\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)\n",
    "\n",
    "# -----------------------------\n",
    "# Load images\n",
    "# -----------------------------\n",
    "def load_geoimage(filename):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    full_path = os.path.join(IMAGES_PATH, filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {full_path}\")\n",
    "\n",
    "    src_raster = rasterio.open(full_path, 'r')\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=np.float32)\n",
    "    for band in range(src_raster.count):\n",
    "        img[:, :, band] = src_raster.read(band + 1)\n",
    "    return img\n",
    "\n",
    "# -----------------------------\n",
    "# Load annotations\n",
    "# -----------------------------\n",
    "json_file = os.path.join(IMAGES_PATH, 'xview_ann_train.json')\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "\n",
    "anns_dataset = []\n",
    "for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n",
    "    image = GenericImage(json_img['filename'])\n",
    "    obj = GenericObject()\n",
    "    obj.bb = tuple(map(int, json_ann['bbox']))\n",
    "    obj.category = json_ann['category_id']  # can be string or int\n",
    "    image.add_object(obj)\n",
    "    anns_dataset.append(image)\n",
    "\n",
    "# -----------------------------\n",
    "# Split dataset\n",
    "# -----------------------------\n",
    "anns_train, anns_valid = train_test_split(\n",
    "    anns_dataset, test_size=0.1, random_state=RANDOM_SEED, shuffle=True\n",
    ")\n",
    "print('Training images:', len(anns_train), 'Validation images:', len(anns_valid))\n",
    "\n",
    "# Flatten annotations\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n",
    "\n",
    "# -----------------------------\n",
    "# Compute class weights (fixed for string categories)\n",
    "# -----------------------------\n",
    "def compute_class_weights(objs):\n",
    "    counts = np.zeros(NUM_CATEGORIES, dtype=np.int64)\n",
    "    for _, obj in objs:\n",
    "        if isinstance(obj.category, str):\n",
    "            cat_idx = category_to_index[obj.category]\n",
    "        else:\n",
    "            cat_idx = int(obj.category)\n",
    "        counts[cat_idx] += 1\n",
    "    counts = np.maximum(counts, 1)  # avoid division by zero\n",
    "    class_weights = {i: float(np.sum(counts)) / (len(counts) * counts[i]) for i in range(len(counts))}\n",
    "    return class_weights, counts\n",
    "\n",
    "class_weights, class_counts = compute_class_weights(objs_train)\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# -----------------------------\n",
    "# Generator (TF-friendly, memory safe)\n",
    "# -----------------------------\n",
    "DOWNSAMPLE_SIZE = (64, 64)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def generator_images(objs, batch_size, do_shuffle=False, target_size=DOWNSAMPLE_SIZE, yield_sample_weight=True):\n",
    "    while True:\n",
    "        if do_shuffle:\n",
    "            np.random.shuffle(objs)\n",
    "        for i in range(0, len(objs), batch_size):\n",
    "            group = objs[i:i+batch_size]\n",
    "            images, labels, sample_weights = [], [], []\n",
    "            for filename, obj in group:\n",
    "                img = load_geoimage(filename).astype(np.float32)\n",
    "\n",
    "                # Ensure 3 channels\n",
    "                if img.ndim == 2:\n",
    "                    img = np.stack([img]*3, axis=-1)\n",
    "                elif img.shape[-1] > 3:\n",
    "                    img = img[..., :3]\n",
    "                elif img.shape[-1] < 3:\n",
    "                    pad_ch = 3 - img.shape[-1]\n",
    "                    pad_shape = list(img.shape)\n",
    "                    pad_shape[-1] = pad_ch\n",
    "                    img = np.concatenate([img, np.zeros(pad_shape, dtype=img.dtype)], axis=-1)\n",
    "\n",
    "                # TF resize and normalize\n",
    "                img_tf = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "                img_tf = tf.image.resize(img_tf, target_size) / 255.0\n",
    "                images.append(img_tf)\n",
    "\n",
    "                # One-hot label\n",
    "                prob = np.zeros(NUM_CATEGORIES, dtype=np.float32)\n",
    "                cat_idx = category_to_index[obj.category] if isinstance(obj.category, str) else int(obj.category)\n",
    "                prob[cat_idx] = 1.0\n",
    "                labels.append(prob)\n",
    "\n",
    "                # Sample weight\n",
    "                if yield_sample_weight:\n",
    "                    sample_weights.append(float(class_weights[cat_idx]))\n",
    "\n",
    "            images = tf.stack(images)\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            if yield_sample_weight:\n",
    "                sample_weights = tf.convert_to_tensor(sample_weights, dtype=tf.float32)\n",
    "                yield images, labels, sample_weights\n",
    "            else:\n",
    "                yield images, labels\n",
    "\n",
    "train_generator = generator_images(objs_train, BATCH_SIZE, do_shuffle=True)\n",
    "valid_generator = generator_images(objs_valid, BATCH_SIZE, do_shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Metal GPU memory growth\n",
    "# -----------------------------\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -----------------------------\n",
    "# FFNN model\n",
    "# -----------------------------\n",
    "model = Sequential([\n",
    "    Input(shape=(DOWNSAMPLE_SIZE[0], DOWNSAMPLE_SIZE[1], 3)),\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(256, kernel_initializer=HeNormal()),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(negative_slope=0.1),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(128, kernel_initializer=HeNormal()),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(negative_slope=0.1),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(NUM_CATEGORIES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -----------------------------\n",
    "# Compile\n",
    "# -----------------------------\n",
    "opt = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# -----------------------------\n",
    "# Callbacks\n",
    "# -----------------------------\n",
    "model_checkpoint = ModelCheckpoint('model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\n",
    "early_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\n",
    "terminate = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "EPOCHS = 20\n",
    "train_steps = math.ceil(len(objs_train)/BATCH_SIZE)\n",
    "valid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_steps,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_idx = int(np.argmax(history.history.get('val_accuracy', [0])))\n",
    "best_value = np.max(history.history.get('val_accuracy', [0]))\n",
    "print(f'Best validation model: epoch {best_idx+1} - val_accuracy {best_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the key changes we tried: \n",
    "\n",
    "1. Downsampling to 112×112: still keeps spatial info but reduces flattened size from 150k → 37k features, cutting the parameter explosion that caused your crashes. If you must keep 224×224 exactly, you can set DOWNSAMPLE_SIZE = (224, 224), but expect much larger memory use and possible OOM.\n",
    "\n",
    "2. He init + BatchNorm + LeakyReLU: works well for dense networks on images — stabilizes training and improves gradient flow.\n",
    "\n",
    "3. Dropout reduces overfitting (you have imbalanced classes and a limited dataset).\n",
    "\n",
    "4. sample_weight: produced per-sample from inverse frequency gives rarer classes more importance during training; this is a simple, generally effective mitigation for your imbalance.\n",
    "\n",
    "5. (specific for M3 GPU): Memory growth helps the TF Metal backend avoid grabbing all GPU memory immediately (reduces crashes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the thing above crashed. trying to do it even safer and smaller below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [ 592 2973 1557 1981  963  632 1118  726 3248  103 1320 1386  272]\n",
      "Class weights: {0: 2.1921777546777546, 1: 0.4365184092732024, 2: 0.8335062496912208, 3: 0.6551081427406515, 4: 1.3476315999680486, 5: 2.0534323271665045, 6: 1.1607953763588825, 7: 1.7875609239245602, 8: 0.399559492231906, 9: 12.599701269604182, 10: 0.9831585081585081, 11: 0.9363414363414363, 12: 4.771210407239819}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 19:49:35.501921: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2025-10-03 19:49:35.501941: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-10-03 19:49:35.501945: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "2025-10-03 19:49:35.501959: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-03 19:49:35.501966: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">221</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m24,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │           \u001b[38;5;34m221\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,357</span> (99.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,357\u001b[0m (99.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,357</span> (99.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,357\u001b[0m (99.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN\n",
    "\n",
    "# -----------------------------\n",
    "# SETUP\n",
    "# -----------------------------\n",
    "IMAGES_PATH = '/Users/anastasia/Desktop/MUIA/CompVis/Project/Data/xview_recognition'\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Categories\n",
    "# -----------------------------\n",
    "categories = {\n",
    "    0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck',\n",
    "    4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator',\n",
    "    8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container',\n",
    "    12: 'Pylon'\n",
    "}\n",
    "NUM_CATEGORIES = len(categories)\n",
    "category_to_index = {v: k for k, v in categories.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# Data classes\n",
    "# -----------------------------\n",
    "class GenericObject:\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category = -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])\n",
    "        self.objects = []\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)\n",
    "\n",
    "# -----------------------------\n",
    "# Load image\n",
    "# -----------------------------\n",
    "def load_geoimage(filename):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    full_path = os.path.join(IMAGES_PATH, filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {full_path}\")\n",
    "\n",
    "    src_raster = rasterio.open(full_path, 'r')\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=np.float32)\n",
    "    for band in range(src_raster.count):\n",
    "        img[:, :, band] = src_raster.read(band + 1)\n",
    "    return img\n",
    "\n",
    "# -----------------------------\n",
    "# Load annotations\n",
    "# -----------------------------\n",
    "json_file = os.path.join(IMAGES_PATH, 'xview_ann_train.json')\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "\n",
    "anns_dataset = []\n",
    "for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n",
    "    image = GenericImage(json_img['filename'])\n",
    "    obj = GenericObject()\n",
    "    obj.bb = tuple(map(int, json_ann['bbox']))\n",
    "    obj.category = json_ann['category_id']\n",
    "    image.add_object(obj)\n",
    "    anns_dataset.append(image)\n",
    "\n",
    "# -----------------------------\n",
    "# Split dataset\n",
    "# -----------------------------\n",
    "anns_train, anns_valid = train_test_split(\n",
    "    anns_dataset, test_size=0.1, random_state=RANDOM_SEED, shuffle=True\n",
    ")\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n",
    "\n",
    "# -----------------------------\n",
    "# Compute class weights\n",
    "# -----------------------------\n",
    "def compute_class_weights(objs):\n",
    "    counts = np.zeros(NUM_CATEGORIES, dtype=np.int64)\n",
    "    for _, obj in objs:\n",
    "        if isinstance(obj.category, str):\n",
    "            cat_idx = category_to_index[obj.category]\n",
    "        else:\n",
    "            cat_idx = int(obj.category)\n",
    "        counts[cat_idx] += 1\n",
    "    counts = np.maximum(counts, 1)\n",
    "    class_weights = {i: float(np.sum(counts)) / (len(counts) * counts[i]) for i in range(len(counts))}\n",
    "    return class_weights, counts\n",
    "\n",
    "class_weights, class_counts = compute_class_weights(objs_train)\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# -----------------------------\n",
    "# Generator\n",
    "# -----------------------------\n",
    "DOWNSAMPLE_SIZE = (16, 16)  # very small to save memory\n",
    "BATCH_SIZE = 2  # tiny batch size to avoid memory spike\n",
    "\n",
    "def generator_images(objs, batch_size=BATCH_SIZE, target_size=DOWNSAMPLE_SIZE):\n",
    "    while True:\n",
    "        np.random.shuffle(objs)\n",
    "        for i in range(0, len(objs), batch_size):\n",
    "            group = objs[i:i+batch_size]\n",
    "            images, labels, sample_weights = [], [], []\n",
    "            for filename, obj in group:\n",
    "                img = load_geoimage(filename).astype(np.float32)\n",
    "                # Resize\n",
    "                img_tf = tf.image.resize(img, target_size)\n",
    "                # Ensure 3 channels\n",
    "                if img_tf.shape[-1] < 3:\n",
    "                    pad_ch = 3 - img_tf.shape[-1]\n",
    "                    img_tf = tf.concat([img_tf, tf.zeros((*img_tf.shape[:-1], pad_ch), dtype=tf.float32)], axis=-1)\n",
    "                elif img_tf.shape[-1] > 3:\n",
    "                    img_tf = img_tf[..., :3]\n",
    "                img_tf = img_tf / 255.0\n",
    "                images.append(img_tf)\n",
    "\n",
    "                # Labels\n",
    "                prob = np.zeros(NUM_CATEGORIES, dtype=np.float32)\n",
    "                prob[category_to_index[obj.category] if isinstance(obj.category, str) else int(obj.category)] = 1.0\n",
    "                labels.append(prob)\n",
    "\n",
    "                # Sample weights\n",
    "                sw = float(class_weights[category_to_index[obj.category] if isinstance(obj.category, str) else int(obj.category)])\n",
    "                sample_weights.append(sw)\n",
    "\n",
    "            yield tf.stack(images), tf.convert_to_tensor(labels, dtype=tf.float32), tf.convert_to_tensor(sample_weights, dtype=tf.float32)\n",
    "\n",
    "train_generator = generator_images(objs_train)\n",
    "valid_generator = generator_images(objs_valid)\n",
    "\n",
    "# -----------------------------\n",
    "# FFNN model (feedforward, partial connections)\n",
    "# -----------------------------\n",
    "model = Sequential([\n",
    "    Input(shape=(DOWNSAMPLE_SIZE[0], DOWNSAMPLE_SIZE[1], 3)),\n",
    "    Flatten(),\n",
    "    Dense(32, kernel_initializer=HeNormal()),\n",
    "    LeakyReLU(0.1),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, kernel_initializer=HeNormal()),\n",
    "    LeakyReLU(0.1),\n",
    "    Dropout(0.1),\n",
    "    Dense(NUM_CATEGORIES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -----------------------------\n",
    "# Compile\n",
    "# -----------------------------\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# -----------------------------\n",
    "# Callbacks\n",
    "# -----------------------------\n",
    "callbacks = [\n",
    "    ModelCheckpoint('ffnn_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau('val_accuracy', factor=0.5, patience=5, verbose=1),\n",
    "    EarlyStopping('val_accuracy', patience=10, verbose=1),\n",
    "    TerminateOnNaN()\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "EPOCHS = 10\n",
    "train_steps = max(1, len(objs_train)//BATCH_SIZE)\n",
    "valid_steps = max(1, len(objs_valid)//BATCH_SIZE)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_steps,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Best validation\n",
    "# -----------------------------\n",
    "best_idx = int(np.argmax(history.history.get('val_accuracy', [0])))\n",
    "best_value = np.max(history.history.get('val_accuracy', [0]))\n",
    "print(f'Best validation model: epoch {best_idx+1} - val_accuracy {best_value:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
