{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13253521,"sourceType":"datasetVersion","datasetId":8398410}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport uuid\nimport math\nimport json\nimport warnings\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import HeNormal\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN\n\n# -----------------------------\n# SETUP\n# -----------------------------\nIMAGES_PATH = '/kaggle/input/xview-recognition'  # Update to Kaggle dataset path\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# -----------------------------\n# Categories\n# -----------------------------\ncategories = {\n    0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck',\n    4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator',\n    8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container',\n    12: 'Pylon'\n}\nNUM_CATEGORIES = len(categories)\ncategory_to_index = {v: k for k, v in categories.items()}\n\n# -----------------------------\n# Data classes\n# -----------------------------\nclass GenericObject:\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category = -1\n        self.score = -1\n\nclass GenericImage:\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])\n        self.objects = []\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)\n\n# -----------------------------\n# Load images with PIL\n# -----------------------------\ndef load_geoimage(filename, target_size=(64,64)):\n    full_path = os.path.join(IMAGES_PATH, filename)\n    img = Image.open(full_path).convert(\"RGB\")  # ensures 3 channels\n    img = img.resize(target_size)\n    img_array = np.array(img).astype(np.float32) / 255.0\n    return img_array\n\n# -----------------------------\n# Load annotations\n# -----------------------------\njson_file = os.path.join(IMAGES_PATH, 'xview_ann_train.json')\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\n\nanns_dataset = []\nfor json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n    image = GenericImage(json_img['filename'])\n    obj = GenericObject()\n    obj.bb = tuple(map(int, json_ann['bbox']))\n    obj.category = json_ann['category_id']\n    image.add_object(obj)\n    anns_dataset.append(image)\n\n# -----------------------------\n# Split dataset\n# -----------------------------\nanns_train, anns_valid = train_test_split(\n    anns_dataset, test_size=0.1, random_state=RANDOM_SEED, shuffle=True\n)\nprint('Training images:', len(anns_train), 'Validation images:', len(anns_valid))\n\n# Flatten annotations\nobjs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\nobjs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n\n# -----------------------------\n# Compute class weights\n# -----------------------------\ndef compute_class_weights(objs):\n    counts = np.zeros(NUM_CATEGORIES, dtype=np.int64)\n    for _, obj in objs:\n        if isinstance(obj.category, str):\n            cat_idx = category_to_index[obj.category]\n        else:\n            cat_idx = int(obj.category)\n        counts[cat_idx] += 1\n    counts = np.maximum(counts, 1)\n    class_weights = {i: float(np.sum(counts)) / (len(counts) * counts[i]) for i in range(len(counts))}\n    return class_weights, counts\n\nclass_weights, class_counts = compute_class_weights(objs_train)\nprint(\"Class counts:\", class_counts)\nprint(\"Class weights:\", class_weights)\n\n# -----------------------------\n# Generator (memory safe)\n# -----------------------------\nDOWNSAMPLE_SIZE = (64, 64)\nBATCH_SIZE = 8\n\ndef generator_images(objs, batch_size=BATCH_SIZE, do_shuffle=False):\n    while True:\n        if do_shuffle:\n            np.random.shuffle(objs)\n        for i in range(0, len(objs), batch_size):\n            group = objs[i:i+batch_size]\n            images, labels, sample_weights = [], [], []\n            for filename, obj in group:\n                img = load_geoimage(filename, target_size=DOWNSAMPLE_SIZE)\n                images.append(img)\n\n                # One-hot label\n                prob = np.zeros(NUM_CATEGORIES, dtype=np.float32)\n                cat_idx = category_to_index[obj.category] if isinstance(obj.category, str) else int(obj.category)\n                prob[cat_idx] = 1.0\n                labels.append(prob)\n\n                # Sample weight\n                sample_weights.append(float(class_weights[cat_idx]))\n\n            images = tf.convert_to_tensor(np.stack(images), dtype=tf.float32)\n            labels = tf.convert_to_tensor(np.stack(labels), dtype=tf.float32)\n            sample_weights = tf.convert_to_tensor(np.stack(sample_weights), dtype=tf.float32)\n            yield images, labels, sample_weights\n\ntrain_generator = generator_images(objs_train, do_shuffle=True)\nvalid_generator = generator_images(objs_valid, do_shuffle=False)\n\n# -----------------------------\n# FFNN Model\n# -----------------------------\nmodel = Sequential([\n    Input(shape=(DOWNSAMPLE_SIZE[0], DOWNSAMPLE_SIZE[1], 3)),\n    Flatten(),\n    Dense(128, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.3),\n    Dense(64, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.2),\n    Dense(NUM_CATEGORIES, activation='softmax')\n])\nmodel.summary()\n\n# -----------------------------\n# Compile\n# -----------------------------\nopt = Adam(learning_rate=1e-3)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# -----------------------------\n# Callbacks\n# -----------------------------\ncallbacks = [\n    ModelCheckpoint('model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n    ReduceLROnPlateau('val_accuracy', factor=0.1, patience=5, verbose=1),\n    EarlyStopping('val_accuracy', patience=10, verbose=1),\n    TerminateOnNaN()\n]\n\n# -----------------------------\n# Training\n# -----------------------------\nEPOCHS = 10\ntrain_steps = math.ceil(len(objs_train)/BATCH_SIZE)\nvalid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_steps,\n    validation_data=valid_generator,\n    validation_steps=valid_steps,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1\n)\n\nbest_idx = int(np.argmax(history.history.get('val_accuracy', [0])))\nbest_value = np.max(history.history.get('val_accuracy', [0]))\nprint(f'Best validation model: epoch {best_idx+1} - val_accuracy {best_value:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T18:50:06.916217Z","iopub.execute_input":"2025-10-03T18:50:06.916804Z","iopub.status.idle":"2025-10-03T19:02:47.042002Z","shell.execute_reply.started":"2025-10-03T18:50:06.916776Z","shell.execute_reply":"2025-10-03T19:02:47.041182Z"}},"outputs":[{"name":"stderr","text":"2025-10-03 18:50:08.367204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759517408.549025      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759517408.603625      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Training images: 16871 Validation images: 1875\nClass counts: [ 592 2973 1557 1981  963  632 1118  726 3248  103 1320 1386  272]\nClass weights: {0: 2.1921777546777546, 1: 0.4365184092732024, 2: 0.8335062496912208, 3: 0.6551081427406515, 4: 1.3476315999680486, 5: 2.0534323271665045, 6: 1.1607953763588825, 7: 1.7875609239245602, 8: 0.399559492231906, 9: 12.599701269604182, 10: 0.9831585081585081, 11: 0.9363414363414363, 12: 4.771210407239819}\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1759517421.405157      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1759517421.405993      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12288\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,572,992\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │           \u001b[38;5;34m845\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12288</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,572,992</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">845</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,582,861\u001b[0m (6.04 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,582,861</span> (6.04 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,582,477\u001b[0m (6.04 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,582,477</span> (6.04 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1759517425.688310     119 service.cc:148] XLA service 0x7f7e5c004520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1759517425.689150     119 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1759517425.689172     119 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1759517425.986688     119 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   4/2109\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 45ms/step - accuracy: 0.2448 - loss: 2.4473","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1759517427.763154     119 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.1807 - loss: 2.3676\nEpoch 1: val_accuracy improved from -inf to 0.26987, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 77ms/step - accuracy: 0.1807 - loss: 2.3675 - val_accuracy: 0.2699 - val_loss: 1.9157 - learning_rate: 0.0010\nEpoch 2/10\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2423 - loss: 2.0159\nEpoch 2: val_accuracy improved from 0.26987 to 0.30293, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 33ms/step - accuracy: 0.2423 - loss: 2.0159 - val_accuracy: 0.3029 - val_loss: 1.8434 - learning_rate: 0.0010\nEpoch 3/10\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2611 - loss: 1.9342\nEpoch 3: val_accuracy did not improve from 0.30293\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 31ms/step - accuracy: 0.2611 - loss: 1.9342 - val_accuracy: 0.2987 - val_loss: 1.7217 - learning_rate: 0.0010\nEpoch 4/10\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2843 - loss: 1.8938\nEpoch 4: val_accuracy improved from 0.30293 to 0.35680, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 30ms/step - accuracy: 0.2843 - loss: 1.8938 - val_accuracy: 0.3568 - val_loss: 1.6559 - learning_rate: 0.0010\nEpoch 5/10\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3078 - loss: 1.8304\nEpoch 5: val_accuracy did not improve from 0.35680\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 30ms/step - accuracy: 0.3078 - loss: 1.8304 - val_accuracy: 0.3371 - val_loss: 1.6741 - learning_rate: 0.0010\nEpoch 6/10\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2914 - loss: 1.8354\nEpoch 6: val_accuracy did not improve from 0.35680\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 30ms/step - accuracy: 0.2914 - loss: 1.8354 - val_accuracy: 0.3515 - val_loss: 1.6387 - learning_rate: 0.0010\nEpoch 7/10\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3131 - loss: 1.8686\nEpoch 7: val_accuracy did not improve from 0.35680\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 30ms/step - accuracy: 0.3131 - loss: 1.8685 - val_accuracy: 0.3467 - val_loss: 1.6457 - learning_rate: 0.0010\nEpoch 8/10\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3186 - loss: 1.7761\nEpoch 8: val_accuracy improved from 0.35680 to 0.37173, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 30ms/step - accuracy: 0.3186 - loss: 1.7761 - val_accuracy: 0.3717 - val_loss: 1.6520 - learning_rate: 0.0010\nEpoch 9/10\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3253 - loss: 1.7728\nEpoch 9: val_accuracy did not improve from 0.37173\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 29ms/step - accuracy: 0.3253 - loss: 1.7728 - val_accuracy: 0.3424 - val_loss: 1.5822 - learning_rate: 0.0010\nEpoch 10/10\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3374 - loss: 1.7320\nEpoch 10: val_accuracy did not improve from 0.37173\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 29ms/step - accuracy: 0.3374 - loss: 1.7320 - val_accuracy: 0.3035 - val_loss: 1.7245 - learning_rate: 0.0010\nBest validation model: epoch 8 - val_accuracy 0.3717\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"The above ffNN is still pretty shitty - in fact more shitty than model 0! best accuracy = 0.37 (M0 was 0.39).\n\nWhat we did next was:\n- Reduce input size for faster training and fewer parameters (32×32)\n\n- Wider and deeper FFNN layers to capture more pixel interactions\n\n- Slightly more aggressive Dropout and BatchNorm\n\n- Lower learning rate for stability\n\n- More epochs with EarlyStopping\n\n- Retain class weights","metadata":{}},{"cell_type":"code","source":"import os\nimport uuid\nimport math\nimport json\nimport warnings\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import HeNormal\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN\n\n# -----------------------------\n# SETUP\n# -----------------------------\nIMAGES_PATH = '/kaggle/input/xview-recognition'  # Kaggle dataset path\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# -----------------------------\n# Categories\n# -----------------------------\ncategories = {\n    0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck',\n    4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator',\n    8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container',\n    12: 'Pylon'\n}\nNUM_CATEGORIES = len(categories)\ncategory_to_index = {v: k for k, v in categories.items()}\n\n# -----------------------------\n# Data classes\n# -----------------------------\nclass GenericObject:\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category = -1\n        self.score = -1\n\nclass GenericImage:\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])\n        self.objects = []\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)\n\n# -----------------------------\n# Load images with PIL\n# -----------------------------\ndef load_geoimage(filename, target_size=(32,32)):\n    full_path = os.path.join(IMAGES_PATH, filename)\n    img = Image.open(full_path).convert(\"RGB\")  # ensures 3 channels\n    img = img.resize(target_size)\n    img_array = np.array(img).astype(np.float32) / 255.0\n    return img_array\n\n# -----------------------------\n# Load annotations\n# -----------------------------\njson_file = os.path.join(IMAGES_PATH, 'xview_ann_train.json')\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\n\nanns_dataset = []\nfor json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n    image = GenericImage(json_img['filename'])\n    obj = GenericObject()\n    obj.bb = tuple(map(int, json_ann['bbox']))\n    obj.category = json_ann['category_id']\n    image.add_object(obj)\n    anns_dataset.append(image)\n\n# -----------------------------\n# Split dataset\n# -----------------------------\nanns_train, anns_valid = train_test_split(\n    anns_dataset, test_size=0.1, random_state=RANDOM_SEED, shuffle=True\n)\nprint('Training images:', len(anns_train), 'Validation images:', len(anns_valid))\n\n# Flatten annotations\nobjs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\nobjs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n\n# -----------------------------\n# Compute class weights\n# -----------------------------\ndef compute_class_weights(objs):\n    counts = np.zeros(NUM_CATEGORIES, dtype=np.int64)\n    for _, obj in objs:\n        cat_idx = category_to_index[obj.category] if isinstance(obj.category, str) else int(obj.category)\n        counts[cat_idx] += 1\n    counts = np.maximum(counts, 1)\n    class_weights = {i: float(np.sum(counts)) / (len(counts) * counts[i]) for i in range(len(counts))}\n    return class_weights, counts\n\nclass_weights, class_counts = compute_class_weights(objs_train)\nprint(\"Class counts:\", class_counts)\nprint(\"Class weights:\", class_weights)\n\n# -----------------------------\n# Generator (memory safe)\n# -----------------------------\nDOWNSAMPLE_SIZE = (32, 32)\nBATCH_SIZE = 8\n\ndef generator_images(objs, batch_size=BATCH_SIZE, do_shuffle=False):\n    while True:\n        if do_shuffle:\n            np.random.shuffle(objs)\n        for i in range(0, len(objs), batch_size):\n            group = objs[i:i+batch_size]\n            images, labels, sample_weights = [], [], []\n            for filename, obj in group:\n                img = load_geoimage(filename, target_size=DOWNSAMPLE_SIZE)\n                images.append(img)\n\n                prob = np.zeros(NUM_CATEGORIES, dtype=np.float32)\n                cat_idx = category_to_index[obj.category] if isinstance(obj.category, str) else int(obj.category)\n                prob[cat_idx] = 1.0\n                labels.append(prob)\n\n                sample_weights.append(float(class_weights[cat_idx]))\n\n            images = tf.convert_to_tensor(np.stack(images), dtype=tf.float32)\n            labels = tf.convert_to_tensor(np.stack(labels), dtype=tf.float32)\n            sample_weights = tf.convert_to_tensor(np.stack(sample_weights), dtype=tf.float32)\n            yield images, labels, sample_weights\n\ntrain_generator = generator_images(objs_train, do_shuffle=True)\nvalid_generator = generator_images(objs_valid, do_shuffle=False)\n\n# -----------------------------\n# FFNN Model (deeper & wider)\n# -----------------------------\nmodel = Sequential([\n    Input(shape=(DOWNSAMPLE_SIZE[0], DOWNSAMPLE_SIZE[1], 3)),\n    Flatten(),\n\n    Dense(512, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.4),\n\n    Dense(256, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.3),\n\n    Dense(128, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.2),\n\n    Dense(NUM_CATEGORIES, activation='softmax')\n])\nmodel.summary()\n\n# -----------------------------\n# Compile\n# -----------------------------\nopt = Adam(learning_rate=1e-4)  # lower LR for stability\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# -----------------------------\n# Callbacks\n# -----------------------------\ncallbacks = [\n    ModelCheckpoint('model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n    ReduceLROnPlateau('val_accuracy', factor=0.1, patience=5, verbose=1),\n    EarlyStopping('val_accuracy', patience=15, verbose=1),\n    TerminateOnNaN()\n]\n\n# -----------------------------\n# Training\n# -----------------------------\nEPOCHS = 30  # more epochs for FFNN\ntrain_steps = math.ceil(len(objs_train)/BATCH_SIZE)\nvalid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_steps,\n    validation_data=valid_generator,\n    validation_steps=valid_steps,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1\n)\n\nbest_idx = int(np.argmax(history.history.get('val_accuracy', [0])))\nbest_value = np.max(history.history.get('val_accuracy', [0]))\nprint(f'Best validation model: epoch {best_idx+1} - val_accuracy {best_value:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:43:59.949991Z","iopub.execute_input":"2025-10-03T19:43:59.950636Z","iopub.status.idle":"2025-10-03T20:12:37.790211Z","shell.execute_reply.started":"2025-10-03T19:43:59.950612Z","shell.execute_reply":"2025-10-03T20:12:37.789619Z"}},"outputs":[{"name":"stdout","text":"Training images: 16871 Validation images: 1875\nClass counts: [ 592 2973 1557 1981  963  632 1118  726 3248  103 1320 1386  272]\nClass weights: {0: 2.1921777546777546, 1: 0.4365184092732024, 2: 0.8335062496912208, 3: 0.6551081427406515, 4: 1.3476315999680486, 5: 2.0534323271665045, 6: 1.1607953763588825, 7: 1.7875609239245602, 8: 0.399559492231906, 9: 12.599701269604182, 10: 0.9831585081585081, 11: 0.9363414363414363, 12: 4.771210407239819}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,573,376\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │         \u001b[38;5;34m1,677\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,376</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,742,861\u001b[0m (6.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,742,861</span> (6.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,741,069\u001b[0m (6.64 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,741,069</span> (6.64 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/30\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1235 - loss: 2.5868\nEpoch 1: val_accuracy improved from -inf to 0.26080, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 41ms/step - accuracy: 0.1235 - loss: 2.5868 - val_accuracy: 0.2608 - val_loss: 1.9348 - learning_rate: 1.0000e-04\nEpoch 2/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2078 - loss: 2.2069\nEpoch 2: val_accuracy improved from 0.26080 to 0.31040, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 29ms/step - accuracy: 0.2078 - loss: 2.2069 - val_accuracy: 0.3104 - val_loss: 1.8200 - learning_rate: 1.0000e-04\nEpoch 3/30\n\u001b[1m2107/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2351 - loss: 2.1028\nEpoch 3: val_accuracy did not improve from 0.31040\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 29ms/step - accuracy: 0.2351 - loss: 2.1028 - val_accuracy: 0.2800 - val_loss: 1.7568 - learning_rate: 1.0000e-04\nEpoch 4/30\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2625 - loss: 2.0197\nEpoch 4: val_accuracy did not improve from 0.31040\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 28ms/step - accuracy: 0.2625 - loss: 2.0197 - val_accuracy: 0.3077 - val_loss: 1.7137 - learning_rate: 1.0000e-04\nEpoch 5/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2786 - loss: 1.9529\nEpoch 5: val_accuracy improved from 0.31040 to 0.31627, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 29ms/step - accuracy: 0.2786 - loss: 1.9529 - val_accuracy: 0.3163 - val_loss: 1.6737 - learning_rate: 1.0000e-04\nEpoch 6/30\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2815 - loss: 1.9238\nEpoch 6: val_accuracy improved from 0.31627 to 0.34400, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 29ms/step - accuracy: 0.2815 - loss: 1.9238 - val_accuracy: 0.3440 - val_loss: 1.6426 - learning_rate: 1.0000e-04\nEpoch 7/30\n\u001b[1m2107/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2753 - loss: 1.9773\nEpoch 7: val_accuracy did not improve from 0.34400\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 28ms/step - accuracy: 0.2753 - loss: 1.9772 - val_accuracy: 0.3312 - val_loss: 1.6548 - learning_rate: 1.0000e-04\nEpoch 8/30\n\u001b[1m2107/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.2965 - loss: 1.8758\nEpoch 8: val_accuracy improved from 0.34400 to 0.35200, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 27ms/step - accuracy: 0.2965 - loss: 1.8758 - val_accuracy: 0.3520 - val_loss: 1.6182 - learning_rate: 1.0000e-04\nEpoch 9/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3054 - loss: 1.8502\nEpoch 9: val_accuracy did not improve from 0.35200\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3054 - loss: 1.8502 - val_accuracy: 0.2789 - val_loss: 1.7084 - learning_rate: 1.0000e-04\nEpoch 10/30\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3075 - loss: 1.8544\nEpoch 10: val_accuracy did not improve from 0.35200\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 26ms/step - accuracy: 0.3075 - loss: 1.8544 - val_accuracy: 0.2955 - val_loss: 1.6673 - learning_rate: 1.0000e-04\nEpoch 11/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3134 - loss: 1.8270\nEpoch 11: val_accuracy improved from 0.35200 to 0.36373, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3134 - loss: 1.8270 - val_accuracy: 0.3637 - val_loss: 1.6323 - learning_rate: 1.0000e-04\nEpoch 12/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3245 - loss: 1.7824\nEpoch 12: val_accuracy did not improve from 0.36373\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3245 - loss: 1.7824 - val_accuracy: 0.3205 - val_loss: 1.6251 - learning_rate: 1.0000e-04\nEpoch 13/30\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3245 - loss: 1.7595\nEpoch 13: val_accuracy did not improve from 0.36373\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3245 - loss: 1.7595 - val_accuracy: 0.3632 - val_loss: 1.5560 - learning_rate: 1.0000e-04\nEpoch 14/30\n\u001b[1m2107/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3320 - loss: 1.7841\nEpoch 14: val_accuracy did not improve from 0.36373\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3320 - loss: 1.7841 - val_accuracy: 0.3232 - val_loss: 1.7053 - learning_rate: 1.0000e-04\nEpoch 15/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3249 - loss: 1.7525\nEpoch 15: val_accuracy did not improve from 0.36373\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3249 - loss: 1.7525 - val_accuracy: 0.3525 - val_loss: 1.5511 - learning_rate: 1.0000e-04\nEpoch 16/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3366 - loss: 1.7246\nEpoch 16: val_accuracy did not improve from 0.36373\n\nEpoch 16: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3366 - loss: 1.7246 - val_accuracy: 0.3307 - val_loss: 1.5952 - learning_rate: 1.0000e-04\nEpoch 17/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3434 - loss: 1.7023\nEpoch 17: val_accuracy did not improve from 0.36373\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3434 - loss: 1.7023 - val_accuracy: 0.3605 - val_loss: 1.5245 - learning_rate: 1.0000e-05\nEpoch 18/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3489 - loss: 1.6904\nEpoch 18: val_accuracy improved from 0.36373 to 0.36800, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 27ms/step - accuracy: 0.3489 - loss: 1.6904 - val_accuracy: 0.3680 - val_loss: 1.5153 - learning_rate: 1.0000e-05\nEpoch 19/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3573 - loss: 1.6414\nEpoch 19: val_accuracy did not improve from 0.36800\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 26ms/step - accuracy: 0.3573 - loss: 1.6414 - val_accuracy: 0.3563 - val_loss: 1.5193 - learning_rate: 1.0000e-05\nEpoch 20/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3586 - loss: 1.6768\nEpoch 20: val_accuracy did not improve from 0.36800\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3586 - loss: 1.6768 - val_accuracy: 0.3643 - val_loss: 1.5046 - learning_rate: 1.0000e-05\nEpoch 21/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3617 - loss: 1.6696\nEpoch 21: val_accuracy did not improve from 0.36800\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3617 - loss: 1.6696 - val_accuracy: 0.3669 - val_loss: 1.5028 - learning_rate: 1.0000e-05\nEpoch 22/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3623 - loss: 1.6336\nEpoch 22: val_accuracy did not improve from 0.36800\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 26ms/step - accuracy: 0.3623 - loss: 1.6336 - val_accuracy: 0.3643 - val_loss: 1.5000 - learning_rate: 1.0000e-05\nEpoch 23/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3564 - loss: 1.6567\nEpoch 23: val_accuracy did not improve from 0.36800\n\nEpoch 23: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 26ms/step - accuracy: 0.3564 - loss: 1.6567 - val_accuracy: 0.3600 - val_loss: 1.5082 - learning_rate: 1.0000e-05\nEpoch 24/30\n\u001b[1m2107/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3634 - loss: 1.6361\nEpoch 24: val_accuracy did not improve from 0.36800\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 26ms/step - accuracy: 0.3634 - loss: 1.6361 - val_accuracy: 0.3616 - val_loss: 1.4939 - learning_rate: 1.0000e-06\nEpoch 25/30\n\u001b[1m2107/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3626 - loss: 1.6259\nEpoch 25: val_accuracy did not improve from 0.36800\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3626 - loss: 1.6259 - val_accuracy: 0.3648 - val_loss: 1.4980 - learning_rate: 1.0000e-06\nEpoch 26/30\n\u001b[1m2107/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3635 - loss: 1.6572\nEpoch 26: val_accuracy did not improve from 0.36800\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 26ms/step - accuracy: 0.3635 - loss: 1.6572 - val_accuracy: 0.3643 - val_loss: 1.5028 - learning_rate: 1.0000e-06\nEpoch 27/30\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3634 - loss: 1.6307\nEpoch 27: val_accuracy did not improve from 0.36800\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3634 - loss: 1.6307 - val_accuracy: 0.3659 - val_loss: 1.4948 - learning_rate: 1.0000e-06\nEpoch 28/30\n\u001b[1m2108/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3708 - loss: 1.6524\nEpoch 28: val_accuracy did not improve from 0.36800\n\nEpoch 28: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3708 - loss: 1.6523 - val_accuracy: 0.3664 - val_loss: 1.4951 - learning_rate: 1.0000e-06\nEpoch 29/30\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3647 - loss: 1.6562\nEpoch 29: val_accuracy improved from 0.36800 to 0.37067, saving model to model.keras\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3647 - loss: 1.6562 - val_accuracy: 0.3707 - val_loss: 1.4976 - learning_rate: 1.0000e-07\nEpoch 30/30\n\u001b[1m2107/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3599 - loss: 1.6061\nEpoch 30: val_accuracy did not improve from 0.37067\n\u001b[1m2109/2109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - accuracy: 0.3599 - loss: 1.6061 - val_accuracy: 0.3616 - val_loss: 1.5018 - learning_rate: 1.0000e-07\nBest validation model: epoch 29 - val_accuracy 0.3707\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Still the model above gives us 0.37 accuracy in validation - which is barely above the expectation by chabce for a 13-class problem.\nNext thing we did:\n- Grayscale downsampled images → reduces input dimensionality.\n- Color histograms → adds simple handcrafted features (optional, but helps FFNN learn better).\n- Brightness and contrast augmentation → basic data augmentation.\n- Wider layers → 1024 → 512 → 256 → 128 → 13.\n- Class weights + oversampling → keep balanced learning.\n- Increase patience because clearly ffNNs struggle with images (especially raw pixels as we saw above)","metadata":{}},{"cell_type":"code","source":"import os\nimport uuid\nimport math\nimport json\nimport numpy as np\nfrom PIL import Image, ImageEnhance\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import HeNormal\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN\n\n# -----------------------------\n# SETUP\n# -----------------------------\nIMAGES_PATH = '/kaggle/input/xview-recognition'\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# -----------------------------\n# Categories\n# -----------------------------\ncategories = {\n    0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck',\n    4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator',\n    8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container',\n    12: 'Pylon'\n}\nNUM_CATEGORIES = len(categories)\ncategory_to_index = {v: k for k, v in categories.items()}\n\n# -----------------------------\n# Data classes\n# -----------------------------\nclass GenericObject:\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category = -1\n        self.score = -1\n\nclass GenericImage:\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])\n        self.objects = []\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)\n\n# -----------------------------\n# Load images (grayscale + augment)\n# -----------------------------\ndef load_image_features(filename, target_size=(32,32), augment=False):\n    full_path = os.path.join(IMAGES_PATH, filename)\n    img = Image.open(full_path).convert(\"RGB\")\n    \n    if augment:\n        img = ImageEnhance.Brightness(img).enhance(np.random.uniform(0.8, 1.2))\n        img = ImageEnhance.Contrast(img).enhance(np.random.uniform(0.8, 1.2))\n    \n    img = img.convert(\"L\")\n    img = img.resize(target_size)\n    img_array = np.array(img).astype(np.float32) / 255.0\n    img_array = img_array.flatten()\n    \n    # Optional histogram (8 bins)\n    hist = np.histogram(img_array, bins=8, range=(0,1))[0].astype(np.float32)\n    hist /= np.sum(hist)\n    \n    features = np.concatenate([img_array, hist])\n    return features\n\n# -----------------------------\n# Load annotations\n# -----------------------------\njson_file = os.path.join(IMAGES_PATH, 'xview_ann_train.json')\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\n\nanns_dataset = []\nfor json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n    image = GenericImage(json_img['filename'])\n    obj = GenericObject()\n    obj.bb = tuple(map(int, json_ann['bbox']))\n    obj.category = json_ann['category_id']\n    \n    # Ensure string categories are mapped to int\n    if isinstance(obj.category, str):\n        obj.category = category_to_index[obj.category]\n    \n    image.add_object(obj)\n    anns_dataset.append(image)\n\n# -----------------------------\n# Split dataset\n# -----------------------------\nanns_train, anns_valid = train_test_split(\n    anns_dataset, test_size=0.1, random_state=RANDOM_SEED, shuffle=True\n)\nprint('Training images:', len(anns_train), 'Validation images:', len(anns_valid))\n\nobjs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\nobjs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n\n# -----------------------------\n# Compute class weights\n# -----------------------------\ndef compute_class_weights(objs):\n    counts = np.zeros(NUM_CATEGORIES, dtype=np.int64)\n    for _, obj in objs:\n        cat_idx = obj.category if isinstance(obj.category, int) else category_to_index[obj.category]\n        counts[cat_idx] += 1\n    counts = np.maximum(counts, 1)\n    class_weights = {i: float(np.sum(counts)) / (len(counts) * counts[i]) for i in range(len(counts))}\n    return class_weights, counts\n\nclass_weights, class_counts = compute_class_weights(objs_train)\nprint(\"Class counts:\", class_counts)\nprint(\"Class weights:\", class_weights)\n\n# -----------------------------\n# Generator\n# -----------------------------\nBATCH_SIZE = 16\n\ndef generator_images(objs, batch_size=BATCH_SIZE, augment=False):\n    while True:\n        np.random.shuffle(objs)\n        for i in range(0, len(objs), batch_size):\n            group = objs[i:i+batch_size]\n            features, labels, sample_weights = [], [], []\n            for filename, obj in group:\n                feat = load_image_features(filename, augment=augment)\n                features.append(feat)\n                \n                prob = np.zeros(NUM_CATEGORIES, dtype=np.float32)\n                prob[obj.category] = 1.0\n                labels.append(prob)\n                sample_weights.append(float(class_weights[obj.category]))\n            \n            X = tf.convert_to_tensor(np.stack(features), dtype=tf.float32)\n            y = tf.convert_to_tensor(np.stack(labels), dtype=tf.float32)\n            sw = tf.convert_to_tensor(np.stack(sample_weights), dtype=tf.float32)\n            yield X, y, sw\n\ntrain_generator = generator_images(objs_train, augment=True)\nvalid_generator = generator_images(objs_valid, augment=False)\n\n# -----------------------------\n# FFNN Model\n# -----------------------------\ninput_dim = 32*32 + 8\nmodel = Sequential([\n    Input(shape=(input_dim,)),\n    Dense(1024, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.4),\n    \n    Dense(512, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.3),\n    \n    Dense(256, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.2),\n    \n    Dense(128, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.2),\n    \n    Dense(NUM_CATEGORIES, activation='softmax')\n])\nmodel.summary()\n\n# -----------------------------\n# Compile\n# -----------------------------\nopt = Adam(learning_rate=1e-3)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# -----------------------------\n# Callbacks\n# -----------------------------\ncallbacks = [\n    ModelCheckpoint('model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n    ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1),\n    EarlyStopping('val_accuracy', patience=20, verbose=1),\n    TerminateOnNaN()\n]\n\n# -----------------------------\n# Training\n# -----------------------------\nEPOCHS = 30\ntrain_steps = math.ceil(len(objs_train)/BATCH_SIZE)\nvalid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_steps,\n    validation_data=valid_generator,\n    validation_steps=valid_steps,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1\n)\n\nbest_idx = int(np.argmax(history.history.get('val_accuracy', [0])))\nbest_value = np.max(history.history.get('val_accuracy', [0]))\nprint(f'Best validation model: epoch {best_idx+1} - val_accuracy {best_value:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T20:29:42.313704Z","iopub.execute_input":"2025-10-03T20:29:42.313977Z","iopub.status.idle":"2025-10-03T21:08:13.058016Z","shell.execute_reply.started":"2025-10-03T20:29:42.313957Z","shell.execute_reply":"2025-10-03T21:08:13.057354Z"}},"outputs":[{"name":"stdout","text":"Training images: 16871 Validation images: 1875\nClass counts: [ 592 2973 1557 1981  963  632 1118  726 3248  103 1320 1386  272]\nClass weights: {0: 2.1921777546777546, 1: 0.4365184092732024, 2: 0.8335062496912208, 3: 0.6551081427406515, 4: 1.3476315999680486, 5: 2.0534323271665045, 6: 1.1607953763588825, 7: 1.7875609239245602, 8: 0.399559492231906, 9: 12.599701269604182, 10: 0.9831585081585081, 11: 0.9363414363414363, 12: 4.771210407239819}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,057,792\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_8 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │         \u001b[38;5;34m1,677\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,057,792</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,756,173\u001b[0m (6.70 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,756,173</span> (6.70 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,752,333\u001b[0m (6.68 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,752,333</span> (6.68 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,840\u001b[0m (15.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> (15.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/30\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.1542 - loss: 2.4340\nEpoch 1: val_accuracy improved from -inf to 0.26667, saving model to model.keras\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 109ms/step - accuracy: 0.1542 - loss: 2.4338 - val_accuracy: 0.2667 - val_loss: 1.8923 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.2537 - loss: 2.0320\nEpoch 2: val_accuracy improved from 0.26667 to 0.32693, saving model to model.keras\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 77ms/step - accuracy: 0.2537 - loss: 2.0320 - val_accuracy: 0.3269 - val_loss: 1.8592 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.2741 - loss: 1.9517\nEpoch 3: val_accuracy improved from 0.32693 to 0.34560, saving model to model.keras\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 76ms/step - accuracy: 0.2741 - loss: 1.9517 - val_accuracy: 0.3456 - val_loss: 1.7410 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.3011 - loss: 1.8816\nEpoch 4: val_accuracy did not improve from 0.34560\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 76ms/step - accuracy: 0.3011 - loss: 1.8817 - val_accuracy: 0.3371 - val_loss: 1.7293 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3216 - loss: 1.8259\nEpoch 5: val_accuracy did not improve from 0.34560\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 75ms/step - accuracy: 0.3216 - loss: 1.8260 - val_accuracy: 0.3109 - val_loss: 1.8719 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.3185 - loss: 1.8348\nEpoch 6: val_accuracy improved from 0.34560 to 0.37173, saving model to model.keras\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 74ms/step - accuracy: 0.3185 - loss: 1.8348 - val_accuracy: 0.3717 - val_loss: 1.7289 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.3278 - loss: 1.7756\nEpoch 7: val_accuracy did not improve from 0.37173\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 73ms/step - accuracy: 0.3278 - loss: 1.7757 - val_accuracy: 0.3547 - val_loss: 1.9020 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3364 - loss: 1.8020\nEpoch 8: val_accuracy did not improve from 0.37173\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 72ms/step - accuracy: 0.3364 - loss: 1.8020 - val_accuracy: 0.3584 - val_loss: 1.6863 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3506 - loss: 1.7625\nEpoch 9: val_accuracy did not improve from 0.37173\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.3506 - loss: 1.7625 - val_accuracy: 0.3653 - val_loss: 1.6167 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3480 - loss: 1.7543\nEpoch 10: val_accuracy improved from 0.37173 to 0.39893, saving model to model.keras\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.3480 - loss: 1.7542 - val_accuracy: 0.3989 - val_loss: 1.6409 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3723 - loss: 1.7203\nEpoch 11: val_accuracy did not improve from 0.39893\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.3723 - loss: 1.7203 - val_accuracy: 0.3755 - val_loss: 1.6179 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3590 - loss: 1.6591\nEpoch 12: val_accuracy did not improve from 0.39893\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 71ms/step - accuracy: 0.3590 - loss: 1.6591 - val_accuracy: 0.3371 - val_loss: 1.6521 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3680 - loss: 1.6877\nEpoch 13: val_accuracy did not improve from 0.39893\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.3680 - loss: 1.6877 - val_accuracy: 0.3760 - val_loss: 1.6434 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3792 - loss: 1.6414\nEpoch 14: val_accuracy did not improve from 0.39893\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 70ms/step - accuracy: 0.3792 - loss: 1.6414 - val_accuracy: 0.3856 - val_loss: 1.6955 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3785 - loss: 1.6317\nEpoch 15: val_accuracy did not improve from 0.39893\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 70ms/step - accuracy: 0.3785 - loss: 1.6317 - val_accuracy: 0.3915 - val_loss: 1.6115 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3850 - loss: 1.6313\nEpoch 16: val_accuracy improved from 0.39893 to 0.41013, saving model to model.keras\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 71ms/step - accuracy: 0.3850 - loss: 1.6312 - val_accuracy: 0.4101 - val_loss: 1.5599 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3783 - loss: 1.6706\nEpoch 17: val_accuracy did not improve from 0.41013\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 71ms/step - accuracy: 0.3783 - loss: 1.6705 - val_accuracy: 0.3413 - val_loss: 1.6885 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3934 - loss: 1.6250\nEpoch 18: val_accuracy improved from 0.41013 to 0.42080, saving model to model.keras\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 71ms/step - accuracy: 0.3934 - loss: 1.6250 - val_accuracy: 0.4208 - val_loss: 1.6401 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3936 - loss: 1.5957\nEpoch 19: val_accuracy did not improve from 0.42080\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 71ms/step - accuracy: 0.3936 - loss: 1.5957 - val_accuracy: 0.3792 - val_loss: 1.6539 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4043 - loss: 1.5521\nEpoch 20: val_accuracy did not improve from 0.42080\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 73ms/step - accuracy: 0.4043 - loss: 1.5521 - val_accuracy: 0.3995 - val_loss: 1.5649 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3985 - loss: 1.5623\nEpoch 21: val_accuracy did not improve from 0.42080\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 71ms/step - accuracy: 0.3985 - loss: 1.5623 - val_accuracy: 0.3957 - val_loss: 1.7126 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4095 - loss: 1.5413\nEpoch 22: val_accuracy did not improve from 0.42080\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.4095 - loss: 1.5413 - val_accuracy: 0.4192 - val_loss: 1.6347 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4085 - loss: 1.5389\nEpoch 23: val_accuracy did not improve from 0.42080\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.4085 - loss: 1.5389 - val_accuracy: 0.4085 - val_loss: 1.6483 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4085 - loss: 1.5250\nEpoch 24: val_accuracy did not improve from 0.42080\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 71ms/step - accuracy: 0.4085 - loss: 1.5250 - val_accuracy: 0.4075 - val_loss: 1.5254 - learning_rate: 0.0010\nEpoch 25/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4157 - loss: 1.5110\nEpoch 25: val_accuracy did not improve from 0.42080\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 71ms/step - accuracy: 0.4157 - loss: 1.5111 - val_accuracy: 0.4149 - val_loss: 1.5783 - learning_rate: 0.0010\nEpoch 26/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4195 - loss: 1.4998\nEpoch 26: val_accuracy improved from 0.42080 to 0.44160, saving model to model.keras\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 71ms/step - accuracy: 0.4195 - loss: 1.4999 - val_accuracy: 0.4416 - val_loss: 1.4789 - learning_rate: 0.0010\nEpoch 27/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4262 - loss: 1.4950\nEpoch 27: val_accuracy did not improve from 0.44160\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.4262 - loss: 1.4950 - val_accuracy: 0.4325 - val_loss: 1.6540 - learning_rate: 0.0010\nEpoch 28/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4273 - loss: 1.4374\nEpoch 28: val_accuracy did not improve from 0.44160\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.4273 - loss: 1.4374 - val_accuracy: 0.4187 - val_loss: 1.7289 - learning_rate: 0.0010\nEpoch 29/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4205 - loss: 1.5199\nEpoch 29: val_accuracy did not improve from 0.44160\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 71ms/step - accuracy: 0.4205 - loss: 1.5198 - val_accuracy: 0.4272 - val_loss: 1.5613 - learning_rate: 0.0010\nEpoch 30/30\n\u001b[1m1054/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4186 - loss: 1.4727\nEpoch 30: val_accuracy did not improve from 0.44160\n\u001b[1m1055/1055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 70ms/step - accuracy: 0.4186 - loss: 1.4727 - val_accuracy: 0.4400 - val_loss: 1.5327 - learning_rate: 0.0010\nBest validation model: epoch 26 - val_accuracy 0.4416\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Ok now we increased accuracy to 0.44.\nNow we introduce the following changes to further boost accuracy:\n- Context padding: crops now include surroundings.\n- Bounding box features: (x1, y1, x2, y2) normalized appended to features.\n- Gaussian noise: injected in preprocessing and in the model (for stability).\n- Model: first layer expanded to 1280 neurons, added GaussianNoise layer.\n- This should give you a more robust, context-aware model - likely better than 0.44 accuracy.","metadata":{}},{"cell_type":"code","source":"import os\nimport uuid\nimport math\nimport json\nimport numpy as np\nfrom PIL import Image, ImageEnhance\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, LeakyReLU, GaussianNoise\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import HeNormal\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN\n\n# -----------------------------\n# SETUP\n# -----------------------------\nIMAGES_PATH = '/kaggle/input/xview-recognition'\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# -----------------------------\n# Categories\n# -----------------------------\ncategories = {\n    0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck',\n    4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator',\n    8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container',\n    12: 'Pylon'\n}\nNUM_CATEGORIES = len(categories)\ncategory_to_index = {v: k for k, v in categories.items()}\n\n# -----------------------------\n# Data classes\n# -----------------------------\nclass GenericObject:\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category = -1\n        self.score = -1\n\nclass GenericImage:\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])\n        self.objects = []\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)\n\n# -----------------------------\n# Load image\n# -----------------------------\ndef load_geoimage(filename):\n    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n    full_path = os.path.join(IMAGES_PATH, filename)\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"Image not found: {full_path}\")\n\n    src_raster = rasterio.open(full_path, 'r')\n    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=np.float32)\n    for band in range(src_raster.count):\n        img[:, :, band] = src_raster.read(band + 1)\n    return img\n\n# -----------------------------\n# Load images with bbox + context\n# -----------------------------\ndef load_image_features(filename, obj, target_size=(32,32), augment=False, context_factor=1.5):\n    full_path = os.path.join(IMAGES_PATH, filename)\n    img = Image.open(full_path).convert(\"RGB\")\n    w, h = img.size\n    \n    # Expand bbox by context factor\n    x1, y1, x2, y2 = obj.bb\n    bw, bh = x2 - x1, y2 - y1\n    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n    pad_w, pad_h = bw * context_factor / 2, bh * context_factor / 2\n    \n    new_x1 = max(0, int(cx - pad_w))\n    new_y1 = max(0, int(cy - pad_h))\n    new_x2 = min(w, int(cx + pad_w))\n    new_y2 = min(h, int(cy + pad_h))\n    \n    crop = img.crop((new_x1, new_y1, new_x2, new_y2))\n    \n    if augment:\n        crop = ImageEnhance.Brightness(crop).enhance(np.random.uniform(0.8, 1.2))\n        crop = ImageEnhance.Contrast(crop).enhance(np.random.uniform(0.8, 1.2))\n    \n    crop = crop.convert(\"L\")\n    crop = crop.resize(target_size)\n    img_array = np.array(crop).astype(np.float32) / 255.0\n    img_array = img_array.flatten()\n    \n    # Optional histogram\n    hist = np.histogram(img_array, bins=8, range=(0,1))[0].astype(np.float32)\n    hist /= np.sum(hist)\n    \n    # Normalized bbox coords\n    norm_bb = np.array([x1/w, y1/h, x2/w, y2/h], dtype=np.float32)\n    \n    # Gaussian noise injection (only in training)\n    if augment:\n        img_array += np.random.normal(0, 0.01, img_array.shape)\n        norm_bb += np.random.normal(0, 0.01, norm_bb.shape)\n    \n    features = np.concatenate([img_array, hist, norm_bb])\n    return features\n\n# -----------------------------\n# Load annotations\n# -----------------------------\njson_file = os.path.join(IMAGES_PATH, 'xview_ann_train.json')\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\n\nanns_dataset = []\nfor json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n    image = GenericImage(json_img['filename'])\n    obj = GenericObject()\n    obj.bb = tuple(map(int, json_ann['bbox']))\n    obj.category = json_ann['category_id']\n    \n    if isinstance(obj.category, str):\n        obj.category = category_to_index[obj.category]\n    \n    image.add_object(obj)\n    anns_dataset.append(image)\n\n# -----------------------------\n# Split dataset\n# -----------------------------\nanns_train, anns_valid = train_test_split(\n    anns_dataset, test_size=0.1, random_state=RANDOM_SEED, shuffle=True\n)\nprint('Training images:', len(anns_train), 'Validation images:', len(anns_valid))\n\nobjs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\nobjs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n\n# -----------------------------\n# Compute class weights\n# -----------------------------\ndef compute_class_weights(objs):\n    counts = np.zeros(NUM_CATEGORIES, dtype=np.int64)\n    for _, obj in objs:\n        cat_idx = obj.category if isinstance(obj.category, int) else category_to_index[obj.category]\n        counts[cat_idx] += 1\n    counts = np.maximum(counts, 1)\n    class_weights = {i: float(np.sum(counts)) / (len(counts) * counts[i]) for i in range(len(counts))}\n    return class_weights, counts\n\nclass_weights, class_counts = compute_class_weights(objs_train)\nprint(\"Class counts:\", class_counts)\nprint(\"Class weights:\", class_weights)\n\n# -----------------------------\n# Generator\n# -----------------------------\nBATCH_SIZE = 16\n\ndef generator_images(objs, batch_size=BATCH_SIZE, augment=False):\n    while True:\n        np.random.shuffle(objs)\n        for i in range(0, len(objs), batch_size):\n            group = objs[i:i+batch_size]\n            features, labels, sample_weights = [], [], []\n            for filename, obj in group:\n                feat = load_image_features(filename, obj, augment=augment)\n                features.append(feat)\n                \n                prob = np.zeros(NUM_CATEGORIES, dtype=np.float32)\n                prob[obj.category] = 1.0\n                labels.append(prob)\n                sample_weights.append(float(class_weights[obj.category]))\n            \n            X = tf.convert_to_tensor(np.stack(features), dtype=tf.float32)\n            y = tf.convert_to_tensor(np.stack(labels), dtype=tf.float32)\n            sw = tf.convert_to_tensor(np.stack(sample_weights), dtype=tf.float32)\n            yield X, y, sw\n\ntrain_generator = generator_images(objs_train, augment=True)\nvalid_generator = generator_images(objs_valid, augment=False)\n\n# -----------------------------\n# FFNN Model (with GaussianNoise layer)\n# -----------------------------\ninput_dim = 32*32 + 8 + 4\nmodel = Sequential([\n    Input(shape=(input_dim,)),\n    GaussianNoise(0.01),  # noise robustness\n    Dense(1280, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.4),\n    \n    Dense(512, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.3),\n    \n    Dense(256, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.2),\n    \n    Dense(128, kernel_initializer=HeNormal()),\n    BatchNormalization(),\n    LeakyReLU(0.1),\n    Dropout(0.2),\n    \n    Dense(NUM_CATEGORIES, activation='softmax')\n])\nmodel.summary()\n\n# -----------------------------\n# Compile\n# -----------------------------\nopt = Adam(learning_rate=1e-3)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# -----------------------------\n# Callbacks\n# -----------------------------\ncallbacks = [\n    ModelCheckpoint('model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n    ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1),\n    EarlyStopping('val_accuracy', patience=20, verbose=1),\n    TerminateOnNaN()\n]\n\n# -----------------------------\n# Training\n# -----------------------------\nEPOCHS = 30\ntrain_steps = math.ceil(len(objs_train)/BATCH_SIZE)\nvalid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_steps,\n    validation_data=valid_generator,\n    validation_steps=valid_steps,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1\n)\n\nbest_idx = int(np.argmax(history.history.get('val_accuracy', [0])))\nbest_value = np.max(history.history.get('val_accuracy', [0]))\nprint(f'Best validation model: epoch {best_idx+1} - val_accuracy {best_value:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T11:51:38.810091Z","iopub.execute_input":"2025-10-05T11:51:38.810397Z"}},"outputs":[{"name":"stderr","text":"2025-10-05 11:51:41.883407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759665102.268296      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759665102.376204      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Training images: 16871 Validation images: 1875\nClass counts: [ 592 2973 1557 1981  963  632 1118  726 3248  103 1320 1386  272]\nClass weights: {0: 2.1921777546777546, 1: 0.4365184092732024, 2: 0.8335062496912208, 3: 0.6551081427406515, 4: 1.3476315999680486, 5: 2.0534323271665045, 6: 1.1607953763588825, 7: 1.7875609239245602, 8: 0.399559492231906, 9: 12.599701269604182, 10: 0.9831585081585081, 11: 0.9363414363414363, 12: 4.771210407239819}\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1759665121.144927      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1759665121.145650      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gaussian_noise (\u001b[38;5;33mGaussianNoise\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1036\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m1,327,360\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │         \u001b[38;5;34m1,677\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gaussian_noise (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1036</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,327,360</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,157,837\u001b[0m (8.23 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,157,837</span> (8.23 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,153,485\u001b[0m (8.21 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,153,485</span> (8.21 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,352\u001b[0m (17.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> (17.00 KB)\n</pre>\n"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"def draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.colormaps['Blues'])\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.show()\n\nmodel.load_weights('model.keras')\ny_true, y_pred = [], []\nfor ann in anns_valid:\n    # Load image\n    image = load_geoimage(ann.filename)\n    for obj_pred in ann.objects:\n        # Generate prediction\n        warped_image = np.expand_dims(image, 0)\n        predictions = model.predict(warped_image, verbose=0)\n        # Save prediction\n        pred_category = list(categories.values())[np.argmax(predictions)]\n        pred_score = np.max(predictions)\n        y_true.append(obj_pred.category)\n        y_pred.append(pred_category)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\ndraw_confusion_matrix(cm, categories)\n\n# Compute the accuracy\ncorrect_samples_class = np.diag(cm).astype(float)\ntotal_samples_class = np.sum(cm, axis=1).astype(float)\ntotal_predicts_class = np.sum(cm, axis=0).astype(float)\nprint('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\nacc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\nprint('Mean Recall: %.3f%%' % (acc.mean() * 100))\nacc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\nprint('Mean Precision: %.3f%%' % (acc.mean() * 100))\nfor idx in range(len(categories)):\n    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n    tp = cm[idx, idx]\n    fp = sum(cm[:, idx]) - tp\n    fn = sum(cm[idx, :]) - tp\n    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n    # Precision: proportion of predicted positive cases that were truly real positives.\n    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}