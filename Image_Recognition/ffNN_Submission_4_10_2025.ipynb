{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "QYuALZOG-AMq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# CV Course Project: Assignment - Image Recognition\n",
    "\n",
    "---\n",
    "\n",
    "**Group:**\n",
    "- Alumno 1:\n",
    "- Alumno 2: Sukhorukova, Anastasia (anastasia.s@alumnos.upm.es)\n",
    "- Alumno 3: Reyes Castro, Didier Yamil (didier.reyes.castro@alumnos.upm.es)\n",
    "\n",
    "**Course:** Computer Vision (CV) - 2025/26\n",
    "\n",
    "**Institution:** Polytechnic University of Madrid (UPM)\n",
    "\n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Goals\n",
    "\n",
    "The goals of the assignment are:\n",
    "* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n",
    "* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n",
    "* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n",
    "* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n",
    "\n",
    "Follow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/2DDPE2zHw5dbM3G](https://drive.upm.es/s/2DDPE2zHw5dbM3G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Google Colab, uncomment the following line to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow numpy rasterio scikit-learn matplotlib keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Conda, create a new environment and install the required packages:\n",
    "```bash\n",
    "conda create -n cv_project python tensorflow numpy rasterio scikit-learn matplotlib jupyterlab\n",
    "conda activate cv_project\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import uuid\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "\n",
    "# External libraries\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Rescaling, BatchNormalization, Dropout, LeakyReLU # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU is available for training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.031186Z",
     "start_time": "2024-10-26T00:00:17.131476Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T08:27:59.622636Z",
     "iopub.status.busy": "2025-09-18T08:27:59.622372Z",
     "iopub.status.idle": "2025-09-18T08:28:13.071424Z",
     "shell.execute_reply": "2025-09-18T08:28:13.070171Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Load the Dataset & Image Loader\n",
    "\n",
    "Before loading the dataset, set the path to the folder containing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = '/mnt/c/Users/didie/Desktop/main/MUIA/CV/Labs/Image_Recognition/xview_recognition/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.066937Z",
     "start_time": "2024-10-26T00:00:21.059126Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:13.076611Z",
     "iopub.status.busy": "2025-09-18T08:28:13.076007Z",
     "iopub.status.idle": "2025-09-18T08:28:13.083344Z",
     "shell.execute_reply": "2025-09-18T08:28:13.082866Z"
    },
    "id": "OYtqD3Oh-AMw",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GenericObject:\n",
    "    \"\"\"\n",
    "    Generic object data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category= -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    \"\"\"\n",
    "    Generic image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n",
    "        self.objects = list([])\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.153693Z",
     "start_time": "2024-10-26T00:00:21.149079Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:13.085587Z",
     "iopub.status.busy": "2025-09-18T08:28:13.085423Z",
     "iopub.status.idle": "2025-09-18T08:28:13.089288Z",
     "shell.execute_reply": "2025-09-18T08:28:13.088555Z"
    },
    "id": "I_GygShu-AMz"
   },
   "outputs": [],
   "source": [
    "categories = {0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck', 4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator', 8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container', 12: 'Pylon'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.292654Z",
     "start_time": "2024-10-26T00:00:21.205321Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:13.091445Z",
     "iopub.status.busy": "2025-09-18T08:28:13.091293Z",
     "iopub.status.idle": "2025-09-18T08:28:13.821931Z",
     "shell.execute_reply": "2025-09-18T08:28:13.820634Z"
    },
    "id": "fRBA7ReQ-AM0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_geoimage(filename):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    src_raster = rasterio.open(IMAGES_PATH + filename, 'r')\n",
    "    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n",
    "    input_type = src_raster.profile['dtype']\n",
    "    input_channels = src_raster.count\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "    for band in range(input_channels):\n",
    "        img[:, :, band] = src_raster.read(band+1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.416449Z",
     "start_time": "2024-10-26T00:00:21.311510Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:13.824938Z",
     "iopub.status.busy": "2025-09-18T08:28:13.824715Z",
     "iopub.status.idle": "2025-09-18T08:28:13.893966Z",
     "shell.execute_reply": "2025-09-18T08:28:13.893581Z"
    },
    "id": "Orto292C-AM3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load database\n",
    "json_file = IMAGES_PATH + 'xview_ann_train.json'\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "ifs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:22.874518Z",
     "start_time": "2024-10-26T00:00:22.204948Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:13.896327Z",
     "iopub.status.busy": "2025-09-18T08:28:13.896163Z",
     "iopub.status.idle": "2025-09-18T08:28:14.116723Z",
     "shell.execute_reply": "2025-09-18T08:28:14.116385Z"
    },
    "id": "4GjFLHs4-AM4",
    "outputId": "5581df22-d4e9-42ac-9f94-061fd8c7acd9"
   },
   "outputs": [],
   "source": [
    "counts = dict.fromkeys(categories.values(), 0)\n",
    "anns_dataset = []\n",
    "for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n",
    "    image = GenericImage(json_img['filename'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    obj.category = json_ann['category_id']\n",
    "    # Resampling strategy to reduce training time\n",
    "    counts[obj.category] += 1\n",
    "    image.add_object(obj)\n",
    "    anns_dataset.append(image)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:16.358521Z",
     "iopub.status.busy": "2025-09-18T08:28:16.358389Z",
     "iopub.status.idle": "2025-09-18T08:28:16.362987Z",
     "shell.execute_reply": "2025-09-18T08:28:16.362279Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_images(objs, batch_size, do_shuffle=False):\n",
    "    while True:\n",
    "        if do_shuffle:\n",
    "            np.random.shuffle(objs)\n",
    "        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n",
    "        for group in groups:\n",
    "            images, labels = [], []\n",
    "            for (filename, obj) in group:\n",
    "                # Load image\n",
    "                images.append(load_geoimage(filename))\n",
    "                probabilities = np.zeros(len(categories))\n",
    "                probabilities[list(categories.values()).index(obj.category)] = 1\n",
    "                labels.append(probabilities)\n",
    "            images = np.array(images).astype(np.float32)\n",
    "            labels = np.array(labels).astype(np.float32)\n",
    "            yield images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "def set_seed(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# Number of categories for classification\n",
    "NUM_CATEGORIES = len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Possible Architectures / Parameters to Tune\n",
    "\n",
    "- Stratification of Split\n",
    "- Normalisation of the input data (rescaling pixels from [0, 255] to [0, 1])\n",
    "- Adam tuning: learning rate, beta 1, beta 2, epsilon, amsgrad\n",
    "- Increase number of epochs\n",
    "- Adding more Dense layers\n",
    "- Change the activation functions of Dense layers: ReLU, leaky ReLU, ELU (which to choose for each layer?)\n",
    "- Change loss function from Crossentropy to Focal Loss\n",
    "- Weight Initialisation (HE initialisation)\n",
    "- Use BatchNorm layers (but where? in all layers?)\n",
    "- Dropout?, Early stopping?\n",
    "- Increase batch size (although it could generalise worst but be parallelised) (also batch size being related to learning rate: learning*N/batch)\n",
    "- Other advanced techniques seen in class: SGD with warm restarts, SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diNBB3qy-AM2"
   },
   "source": [
    "## 1 Model 0: Simple (and Bad) ffNN > Multinomial Logistic Regression Model\n",
    "\n",
    "The following architecture is used as a starting point:\n",
    "- Validation split: 10% (NO stratification)\n",
    "- Input layer: 150,528 neurons (224x224x3) > Activation function: ReLU\n",
    "- Output layer: 13 neurons (the categories to classify) > Activation function: Softmax\n",
    "- Optimiser: Adam with learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0\n",
    "- Loss function: Categorical Crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:23.656800Z",
     "start_time": "2024-10-26T00:00:23.123245Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:14.118629Z",
     "iopub.status.busy": "2025-09-18T08:28:14.118484Z",
     "iopub.status.idle": "2025-09-18T08:28:15.638109Z",
     "shell.execute_reply": "2025-09-18T08:28:15.637081Z"
    },
    "id": "NriAECvS-AM6"
   },
   "outputs": [],
   "source": [
    "anns_train, anns_valid = train_test_split(anns_dataset, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)\n",
    "print('Number of training images: ' + str(len(anns_train)))\n",
    "print('Number of validation images: ' + str(len(anns_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:25.056806Z",
     "start_time": "2024-10-26T00:00:24.261581Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:15.641605Z",
     "iopub.status.busy": "2025-09-18T08:28:15.641261Z",
     "iopub.status.idle": "2025-09-18T08:28:16.335748Z",
     "shell.execute_reply": "2025-09-18T08:28:16.335083Z"
    },
    "id": "BNkjbY2e-AM7",
    "outputId": "47bde031-306f-464e-8e22-cc70a7fb7c67"
   },
   "outputs": [],
   "source": [
    "print('Compiling the model...')\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)), # 224x224 images with 3 channels (RGB)\n",
    "    Flatten(), # Flatten the 3D tensor to 1D\n",
    "    Activation('relu'),\n",
    "    Dense(NUM_CATEGORIES, activation='softmax') # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are helper functions that monitor the training process:\n",
    "- **ModelCheckpoint:** Save the best model based on validation accuracy.\n",
    "- **ReduceLROnPlateau:** Reduce learning rate when validation accuracy stops improving.\n",
    "- **EarlyStopping:** Stop training when validation accuracy stops improving.\n",
    "- **TerminateOnNaN:** Stop training if NaN loss is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:26.254555Z",
     "start_time": "2024-10-26T00:00:26.243908Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:16.352904Z",
     "iopub.status.busy": "2025-09-18T08:28:16.352762Z",
     "iopub.status.idle": "2025-09-18T08:28:16.356742Z",
     "shell.execute_reply": "2025-09-18T08:28:16.356290Z"
    },
    "id": "GGAJEfpB-AM8"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_checkpoint = ModelCheckpoint('model.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\n",
    "early_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\n",
    "terminate = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the ImageObjects to actual image generators for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:27.058834Z",
     "start_time": "2024-10-26T00:00:27.022627Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:16.364748Z",
     "iopub.status.busy": "2025-09-18T08:28:16.364617Z",
     "iopub.status.idle": "2025-09-18T08:28:16.378947Z",
     "shell.execute_reply": "2025-09-18T08:28:16.377825Z"
    },
    "id": "Yht-QqUH-AM8"
   },
   "outputs": [],
   "source": [
    "# Generate the list of objects from annotations\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n",
    "\n",
    "# Generators\n",
    "BATCH_SIZE = 16\n",
    "train_generator = generator_images(objs_train, BATCH_SIZE, do_shuffle=True)\n",
    "valid_generator = generator_images(objs_valid, BATCH_SIZE, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-26T00:00:27.913670Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:16.382264Z",
     "iopub.status.busy": "2025-09-18T08:28:16.382013Z",
     "iopub.status.idle": "2025-09-18T12:58:23.599009Z",
     "shell.execute_reply": "2025-09-18T12:58:23.597652Z"
    },
    "id": "TrfpdECs-AM9",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "21d89b78-d94c-442e-9bc2-517654c0b614",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Training model...')\n",
    "\n",
    "EPOCHS = 20\n",
    "train_steps = math.ceil(len(objs_train)/BATCH_SIZE)\n",
    "valid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n",
    "\n",
    "h = model.fit(train_generator, \n",
    "              steps_per_epoch=train_steps, \n",
    "              validation_data=valid_generator, \n",
    "              validation_steps=valid_steps, \n",
    "              epochs=EPOCHS, \n",
    "              callbacks=callbacks, \n",
    "              verbose=1)\n",
    "\n",
    "# Best validation model\n",
    "best_idx = int(np.argmax(h.history['val_accuracy']))\n",
    "best_value = np.max(h.history['val_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training time: ~ 108 minutes\n",
    "- GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
    "- Best validation model: Epoch 15 - Validation accuracy: 0.3941"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "8IMMO_mT-AM9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1.3 Validation\n",
    "Compute validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:58:23.603258Z",
     "iopub.status.busy": "2025-09-18T12:58:23.602956Z",
     "iopub.status.idle": "2025-09-18T12:58:24.340498Z",
     "shell.execute_reply": "2025-09-18T12:58:24.339307Z"
    },
    "id": "HAanJ-V0-AM1"
   },
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(cm, categories):\n",
    "    # Draw confusion matrix\n",
    "    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n",
    "    ax = fig.add_subplot(111)\n",
    "    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.colormaps['Blues'])\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:58:24.345003Z",
     "iopub.status.busy": "2025-09-18T12:58:24.344477Z",
     "iopub.status.idle": "2025-09-18T13:01:22.017111Z",
     "shell.execute_reply": "2025-09-18T13:01:22.016333Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.keras')\n",
    "y_true, y_pred = [], []\n",
    "for ann in anns_valid:\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename)\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        y_true.append(obj_pred.category)\n",
    "        y_pred.append(pred_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:22.019704Z",
     "iopub.status.busy": "2025-09-18T13:01:22.019551Z",
     "iopub.status.idle": "2025-09-18T13:01:22.740150Z",
     "shell.execute_reply": "2025-09-18T13:01:22.739387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n",
    "draw_confusion_matrix(cm, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:22.742591Z",
     "iopub.status.busy": "2025-09-18T13:01:22.742443Z",
     "iopub.status.idle": "2025-09-18T13:01:22.750853Z",
     "shell.execute_reply": "2025-09-18T13:01:22.749931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy\n",
    "correct_samples_class = np.diag(cm).astype(float)\n",
    "total_samples_class = np.sum(cm, axis=1).astype(float)\n",
    "total_predicts_class = np.sum(cm, axis=0).astype(float)\n",
    "print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n",
    "acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n",
    "print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n",
    "acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n",
    "print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n",
    "for idx in range(len(categories)):\n",
    "    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n",
    "    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n",
    "    tp = cm[idx, idx]\n",
    "    fp = sum(cm[:, idx]) - tp\n",
    "    fn = sum(cm[idx, :]) - tp\n",
    "    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n",
    "    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n",
    "    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n",
    "    # Precision: proportion of predicted positive cases that were truly real positives.\n",
    "    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n",
    "    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n",
    "    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n",
    "    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n",
    "    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n",
    "    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n",
    "    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testing\n",
    "Trying to improve the results provided in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:22.753680Z",
     "iopub.status.busy": "2025-09-18T13:01:22.753432Z",
     "iopub.status.idle": "2025-09-18T13:01:22.832351Z",
     "shell.execute_reply": "2025-09-18T13:01:22.831293Z"
    },
    "id": "tJr_-xCt-AM-"
   },
   "outputs": [],
   "source": [
    "anns = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk('/mnt/c/Users/didie/Desktop/main/MUIA/CV/Labs/Image_Recognition/xview_recognition/xview_test'):\n",
    "    for filename in filenames:\n",
    "        image = GenericImage(dirpath + '/' + filename)\n",
    "        image.tile = np.array([0, 0, 224, 224])\n",
    "        obj = GenericObject()\n",
    "        obj.bb = (0, 0, 224, 224)\n",
    "        obj.category = dirpath[dirpath.rfind('/')+1:]\n",
    "        image.add_object(obj)\n",
    "        anns.append(image)\n",
    "print('Number of testing images: ' + str(len(anns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:22.837502Z",
     "iopub.status.busy": "2025-09-18T13:01:22.837113Z",
     "iopub.status.idle": "2025-09-18T13:03:24.835373Z",
     "shell.execute_reply": "2025-09-18T13:03:24.834773Z"
    },
    "id": "TGs2zqfv-AM_"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model_5.keras')\n",
    "predictions_data = {\"images\": {}, \"annotations\": {}}\n",
    "\n",
    "i = 0\n",
    "\n",
    "for idx, ann in enumerate(anns):\n",
    "    image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n",
    "    predictions_data[\"images\"][idx] = image_data\n",
    "\n",
    "    if i == 0:\n",
    "        print(image_data)\n",
    "        i += 1\n",
    "\n",
    "    # Load image\n",
    "    print(ann.filename)\n",
    "    image = load_geoimage(ann.filename)\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n",
    "        predictions_data[\"annotations\"][idx] = annotation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:03:24.837935Z",
     "iopub.status.busy": "2025-09-18T13:03:24.837770Z",
     "iopub.status.idle": "2025-09-18T13:03:24.871502Z",
     "shell.execute_reply": "2025-09-18T13:03:24.870824Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"prediction.json\", \"w\") as outfile:\n",
    "    json.dump(predictions_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model Adjustments\n",
    "\n",
    "The following architecture is used:\n",
    "- Validation split: 10% (stratified)\n",
    "- Input layer: The images are rescaled from [0, 255] to [0, 1]\n",
    "- Two hidden layers:\n",
    "  - Hidden layer 1: 256 neurons > He initialisation > Activation function: Leaky ReLU\n",
    "  - Hidden layer 2: 128 neurons > He initialisation > Activation function: Leaky ReLU\n",
    "- Batch Normalization after each hidden layer\n",
    "- Dropout after each hidden layer\n",
    "- Loss function: Focal Loss\n",
    "- Optimiser: Adam with learning_rate=1e-3\n",
    "\n",
    "Future:\n",
    "- Modify current architecture: More neurons or more layers\n",
    "- Increase number of epochs\n",
    "- Increase batch size (could generalise worst)\n",
    "- Try SAM OR SGD with warm restarts\n",
    "- Modify current parameters: validation split size, not stratifying but shuffling data, Adam parameters, dropout rate, leaky ReLU slope, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, val2 = train_test_split(anns_dataset,\n",
    "                                test_size=0.1, \n",
    "                                random_state=RANDOM_SEED, \n",
    "                                stratify=[obj.category for ann in anns_dataset for obj in ann.objects])\n",
    "print('Number of training images: ' + str(len(train2)))\n",
    "print('Number of validation images: ' + str(len(val2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from collections import Counter\n",
    "\n",
    "# 1. Extract the category labels from your datasets\n",
    "#train_labels = [obj.category for ann in train2 for obj in ann.objects]\n",
    "#valid_labels = [obj.category for ann in val2 for obj in ann.objects]\n",
    "\n",
    "# 2. Count the occurrences of each category\n",
    "#train_counts = Counter(train_labels)\n",
    "#valid_counts = Counter(valid_labels)\n",
    "\n",
    "# 3. Print the counts and proportions\n",
    "#print(\"--- Training Set Proportions ---\")\n",
    "#for cat, count in sorted(train_counts.items()):\n",
    "#    percentage = (count / len(train_labels)) * 100\n",
    "#    print(f\"{cat} | Count: {count:<5} | Percentage: {percentage:.2f}%\")\n",
    "\n",
    "#print(\"\\n--- Validation Set Proportions ---\")\n",
    "#for cat, count in sorted(valid_counts.items()):\n",
    "#    percentage = (count / len(valid_labels)) * 100\n",
    "#    print(f\"{cat} | Count: {count:<5} | Percentage: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Compiling the model...')\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)), # 224x224 images with 3 channels (RGB)\n",
    "    Rescaling(1./255), # Rescale pixel values to [0, 1]\n",
    "    Flatten(), # Flatten the 3D tensor to 1D\n",
    "    \n",
    "    Dense(256, kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(negative_slope=0.1),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(128, kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(negative_slope=0.1),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(NUM_CATEGORIES, activation='softmax') # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\n",
    "model.compile(optimizer=opt, loss='categorical_focal_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "MODEL_NAME = 'model_5.keras'\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(MODEL_NAME, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=40, verbose=1)\n",
    "terminate = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of objects from annotations\n",
    "objs_train = [(ann.filename, obj) for ann in train2 for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in val2 for obj in ann.objects]\n",
    "\n",
    "# Generators\n",
    "BATCH_SIZE = 16\n",
    "train_generator = generator_images(objs_train, BATCH_SIZE, do_shuffle=True)\n",
    "valid_generator = generator_images(objs_valid, BATCH_SIZE, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training model...')\n",
    "\n",
    "EPOCHS = 20\n",
    "train_steps = math.ceil(len(objs_train)/BATCH_SIZE)\n",
    "valid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n",
    "\n",
    "h1 = model.fit(train_generator, \n",
    "              steps_per_epoch=train_steps, \n",
    "              validation_data=valid_generator, \n",
    "              validation_steps=valid_steps, \n",
    "              epochs=EPOCHS, \n",
    "              callbacks=callbacks, \n",
    "              verbose=1)\n",
    "\n",
    "# Best validation model\n",
    "best_idx = int(np.argmax(h1.history['val_accuracy']))\n",
    "best_value = np.max(h1.history['val_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training time: ~101  minutes\n",
    "- GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
    "- Best validation model: Epoch 17  - Validation accuracy: 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(cm, categories):\n",
    "    # Draw confusion matrix\n",
    "    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n",
    "    ax = fig.add_subplot(111)\n",
    "    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.colormaps['Blues'])\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_5.keras')\n",
    "y_true, y_pred = [], []\n",
    "for ann in val2:\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename)\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        y_true.append(obj_pred.category)\n",
    "        y_pred.append(pred_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n",
    "draw_confusion_matrix(cm, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy\n",
    "correct_samples_class = np.diag(cm).astype(float)\n",
    "total_samples_class = np.sum(cm, axis=1).astype(float)\n",
    "total_predicts_class = np.sum(cm, axis=0).astype(float)\n",
    "print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n",
    "acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n",
    "print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n",
    "acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n",
    "print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n",
    "for idx in range(len(categories)):\n",
    "    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n",
    "    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n",
    "    tp = cm[idx, idx]\n",
    "    fp = sum(cm[:, idx]) - tp\n",
    "    fn = sum(cm[idx, :]) - tp\n",
    "    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n",
    "    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n",
    "    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n",
    "    # Precision: proportion of predicted positive cases that were truly real positives.\n",
    "    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n",
    "    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n",
    "    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n",
    "    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n",
    "    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n",
    "    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n",
    "    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = []\n",
    "i = 0\n",
    "for (dirpath, dirnames, filenames) in os.walk('/mnt/c/Users/didie/Desktop/main/MUIA/CV/Labs/Image_Recognition/xview_recognition/xview_test/'):\n",
    "    for filename in filenames:\n",
    "        image = GenericImage(dirpath + '/' + filename)\n",
    "        image.tile = np.array([0, 0, 224, 224])\n",
    "        obj = GenericObject()\n",
    "        obj.bb = (0, 0, 224, 224)\n",
    "        obj.category = dirpath[dirpath.rfind('/')+1:]\n",
    "        image.add_object(obj)\n",
    "        anns.append(image)\n",
    "print('Number of testing images: ' + str(len(anns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_5.keras')\n",
    "predictions_data = {\"images\": {}, \"annotations\": {}}\n",
    "\n",
    "for idx, ann in enumerate(anns):\n",
    "    image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n",
    "    predictions_data[\"images\"][idx] = image_data\n",
    "    # Load image\n",
    "    image = load_geoimage(\"/\".join(ann.filename.split('/')[-3:]))\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n",
    "        predictions_data[\"annotations\"][idx] = annotation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prediction.json\", \"w\") as outfile:\n",
    "    json.dump(predictions_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Log of Changes and Results\n",
    "\n",
    "- Model 0 > Model 1\n",
    "\n",
    "It seems like the model is performing worse (0.2176) than the previous one (0.36)  and almost always predicting \"small car\" for every image. Next steps: Decrease learning rate (1e-4)\n",
    "\n",
    "- Model 1 > Model 2\n",
    "\n",
    "The model performance has improved significantly (0.5093 of accuracy). Let's now try to add BatchNorm layers and Dropout to see if we can improve it even more.\n",
    "\n",
    "- Model 2 > Model 3\n",
    "\n",
    "The model performance has improved a little bit more (0.5530 of accuracy). The advantages of BatchNorm are that it allows for higher learning rates, so let's try increase back to 1e-3 and see if we can improve it even more.\n",
    "\n",
    "- Model 3 > Model 4\n",
    "\n",
    "The performance has improved slightly (0.5589) but the training time has decreased (110 min to 100 min). Let's now try to change the activation functions to Leaky ReLU (default: 0.3 negative slope) and see if we can improve it even more.\n",
    "\n",
    "- Model 4 > Model 5\n",
    "\n",
    "The performance was 0.5440 with 98 minutes, slightly worse than before. As there is class imbalance, let's try to change the loss function from Crossentropy to Focal Loss. Also, let's try to decrease the negative slope of Leaky ReLU from 0.3 (default) to 0.1.\n",
    "\n",
    "- Model 5\n",
    "\n",
    "The performance has improved a little bit more (0.6000) from Model 4 with a training time of 99 minutes.\n",
    "\n",
    "**TODO: ASK DIDIER FOR IMAGES ABOUT THESE RESULTS FOR POTENTIAL PRESENTATION!!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
